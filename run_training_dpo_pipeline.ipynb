{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masterlyj/self-LLM_Agent_RL/blob/main/run_training_dpo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "C84bl6h-i0BW"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "EAACx8I6i0BX"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5zc5Hjei0BY"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ERBsSgi0BY"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXzRiD_3i0BY",
        "outputId": "618a8509-216c-4fce-d93a-f7b68326b5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 99 (delta 19), reused 52 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (99/99), 8.98 MiB | 15.04 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   reward_modeling.py\n",
            "chatpdf.py                  \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "CITATION.cff                run_dpo.sh\n",
            "_config.yml                 run_eval_quantize.sh\n",
            "CONTRIBUTING.md             run_full_sft.sh\n",
            "convert_dataset.py          run_grpo.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo_xm_test.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n",
            "requirements.txt\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Collecting loguru (from -r requirements.txt (line 3))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.18.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.6)\n",
            "Collecting trl>=0.15.2 (from -r requirements.txt (line 10))\n",
            "  Downloading trl-0.27.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.11.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting math-verify==0.5.2 (from -r requirements.txt (line 13))\n",
            "  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended->-r requirements.txt (line 12))\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.22.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2026.1.4)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.22.0)\n",
            "Downloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n",
            "Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.27.0-py3-none-any.whl (532 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: antlr4-python3-runtime, loguru, latex2sympy2_extended, math-verify, trl\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2 latex2sympy2_extended-1.0.6 loguru-0.7.3 math-verify-0.5.2 trl-0.27.0\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btG4dAIVi0BZ"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8osjMqGfi0BZ",
        "outputId": "de2e5de3-c861-4096-d8dc-c38f5bf2e325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKkcvjWyi0BZ",
        "outputId": "803b2f98-c663-4358-fded-32ddff9f694a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 05:27:53.640607: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769146073.660074    3657 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769146073.666170    3657 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769146073.681287    3657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146073.681313    3657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146073.681317    3657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146073.681320    3657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 05:27:53.686023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-01-23 05:27:59.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:27:59.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True, packing=True)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:27:59.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=no,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Jan23_05-27-59_e9f480ff6040,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs=None,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "project=huggingface,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "trackio_space_id=trackio,\n",
            "use_cpu=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2026-01-23 05:27:59.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:27:59.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 7.23kB [00:00, 28.8MB/s]\n",
            "vocab.json: 2.78MB [00:00, 73.5MB/s]\n",
            "merges.txt: 1.67MB [00:00, 117MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 184MB/s]\n",
            "\u001b[32m2026-01-23 05:28:00.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m495\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/tianlongbabu.txt', './data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt']\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:00.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m505\u001b[0m - \u001b[1meval files: ['./data/pretrain/tianlongbabu.txt', './data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 179913.04 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 260551.68 examples/s]\n",
            "\u001b[32m2026-01-23 05:28:00.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 379.92 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 385.21 examples/s]\n",
            "Packing texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 10022.91 examples/s]\n",
            "Packing texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9804.92 examples/s]\n",
            "\u001b[32m2026-01-23 05:28:32.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m600\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2532\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:32.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m601\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:32.179\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m602\u001b[0m - \u001b[34m\u001b[1m天龙八部\n",
            "<|endoftext|>\n",
            "<|endoftext|>\n",
            "<|endoftext|>正文 释名\n",
            "<|endoftext|>“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "<|endoftext|>“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:32.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m614\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:32.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m615\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:32.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m616\u001b[0m - \u001b[34m\u001b[1m天龙八部\n",
            "<|endoftext|>\n",
            "<|endoftext|>\n",
            "<|endoftext|>正文 释名\n",
            "<|endoftext|>“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "<|endoftext|>“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部\u001b[0m\n",
            "config.json: 100% 681/681 [00:00<00:00, 7.21MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 988M/988M [00:06<00:00, 150MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 1.01MB/s]\n",
            "\u001b[32m2026-01-23 05:28:39.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m675\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:39.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m680\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:39.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m693\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:39.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m694\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/content/MedicalGPT/pretraining.py:724: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2026-01-23 05:28:40.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m738\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2026-01-23 05:28:40.392\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m739\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 20221, 101002, 101320,     19,     15,  24300,  57191,     19,     16,\n",
            "          24300,   1773, 103202,  28946,  42140,  17714,     20, 115465, 101939,\n",
            "           3837, 113137, 111007, 104595, 101107,   3837,  42140,  17714, 101474,\n",
            "          52510,  90885,  99364,   3837, 102443,  94498,   8863,  32571,   3837,\n",
            "          99595,  99285,  99259,  81217, 113052,   1773,  99659, 101924,  62112,\n",
            "         113137,   5373, 113052, 100673,  99694,  52510,   3837, 106506, 101120,\n",
            "          28946,  30440,  62112, 108246,  49238,  99178, 117489,  68536, 102161,\n",
            "           1773, 102800, 101924,  97706, 115563, 103162, 100406,   5373, 109244,\n",
            "          49567, 115610, 101304, 111492,   1773, 100859, 101002, 115275,  30440,\n",
            "         100771, 100859,  38176, 101232, 114814,  36885, 100439,   3837, 101107,\n",
            "         113080, 121033, 101143, 100439,   3837, 103972,  30440, 100771, 114225,\n",
            "         100859,  99619, 101684,   1773,  99556,  54021,     17,     17,  42973,\n",
            "             21,     16, 130696, 114225, 100859,  99619, 101684, 103095,  18830,\n",
            "         100859, 101002, 115275,   9370, 104918,   1773, 108976, 102119, 100652,\n",
            "             17,     93],\n",
            "        [ 30534,  56568,  36407, 100349, 113974, 100349, 119454,  81264,  44636,\n",
            "          99294, 100396,  99476,  79766,  99476,  79766,   9370,  44793,  36987,\n",
            "          37474, 106292, 106753, 107575,   3837, 105043, 111897,  38182,  64355,\n",
            "           3837, 100090,  38182,  99235,   9370,   3837, 104984,  99172, 100921,\n",
            "         101686,  81264, 102903, 119516,  99315, 112377,  31935,  99425,   3837,\n",
            "         104543,  44793,  36987,  56568,  12210,   3837, 109978,  16530, 100921,\n",
            "           1773, 109978, 100644,  80158, 108629,  99487, 111542,  42192,  39973,\n",
            "           9370,  99235,  56568,   1773, 109978,  14777, 111488,   3837, 100090,\n",
            "          34187,  43288, 108634,  17714,  99235,   3837, 105500,  74763, 105500,\n",
            "         105382,  32945, 105407, 108226,  36587,  69249,   3837,  42192, 105378,\n",
            "          98650, 119988,  62945,   8997, 151643, 101953,  99243, 101269,  44793,\n",
            "          36987,  75061, 100203,  35568,   3837,  53222,  57218,  16530,  53222,\n",
            "           3837,  77288, 100284, 102557, 107682,  77144,  32945,  75061,  31207,\n",
            "         100956, 106052,  36987,  53222,   3837,  53222,   3837,  53222,   6313,\n",
            "          99795,  53222],\n",
            "        [ 32945, 105997,  36587,   3837, 105997,  99723,  95256, 100306,  26939,\n",
            "          44636,  99294, 100396,  95256,  24562,   3837,  99882,  44793,  36987,\n",
            "         118680,  36667,  55806,   3837,  99200, 100040,  52129,  24156,  32945,\n",
            "         116770,  14777, 119460,   3837, 100994,  26939,  75061, 105700, 111851,\n",
            "           3837, 100502, 111332,  36987, 102000,   3837,  56568, 109828,  30440,\n",
            "          52801,  81264,  75061, 105700,  44793,  36987, 104139, 101132,  81264,\n",
            "          37474,  36556, 112862,  94443,  44934,  14777,  63367,   3837, 110530,\n",
            "          42192,  22226,   3837,  36667,  27442,  15946,  34187,  99517, 102113,\n",
            "          64689,  14009,  44928,  64689, 102811,    527,   1773,  75061, 105700,\n",
            "         119238, 100992,  99287,   3837,  99364,  91676,  64272,  99805,   1773,\n",
            "          37474,  36556, 112862, 100867, 110651, 106855, 104935,  99517,   3837,\n",
            "          99436,  19403,  99851, 102838,   3837,  99882,  44793,  36987, 103924,\n",
            "         111447,   6313, 102000,   3837,  56568,  99387,  14053,  99494, 104060,\n",
            "          81264,    198, 151643, 101141,  99425, 102430,  16530, 109845,  18830,\n",
            "         101564,   3837]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 20221, 101002, 101320,     19,     15,  24300,  57191,     19,     16,\n",
            "          24300,   1773, 103202,  28946,  42140,  17714,     20, 115465, 101939,\n",
            "           3837, 113137, 111007, 104595, 101107,   3837,  42140,  17714, 101474,\n",
            "          52510,  90885,  99364,   3837, 102443,  94498,   8863,  32571,   3837,\n",
            "          99595,  99285,  99259,  81217, 113052,   1773,  99659, 101924,  62112,\n",
            "         113137,   5373, 113052, 100673,  99694,  52510,   3837, 106506, 101120,\n",
            "          28946,  30440,  62112, 108246,  49238,  99178, 117489,  68536, 102161,\n",
            "           1773, 102800, 101924,  97706, 115563, 103162, 100406,   5373, 109244,\n",
            "          49567, 115610, 101304, 111492,   1773, 100859, 101002, 115275,  30440,\n",
            "         100771, 100859,  38176, 101232, 114814,  36885, 100439,   3837, 101107,\n",
            "         113080, 121033, 101143, 100439,   3837, 103972,  30440, 100771, 114225,\n",
            "         100859,  99619, 101684,   1773,  99556,  54021,     17,     17,  42973,\n",
            "             21,     16, 130696, 114225, 100859,  99619, 101684, 103095,  18830,\n",
            "         100859, 101002, 115275,   9370, 104918,   1773, 108976, 102119, 100652,\n",
            "             17,     93],\n",
            "        [ 30534,  56568,  36407, 100349, 113974, 100349, 119454,  81264,  44636,\n",
            "          99294, 100396,  99476,  79766,  99476,  79766,   9370,  44793,  36987,\n",
            "          37474, 106292, 106753, 107575,   3837, 105043, 111897,  38182,  64355,\n",
            "           3837, 100090,  38182,  99235,   9370,   3837, 104984,  99172, 100921,\n",
            "         101686,  81264, 102903, 119516,  99315, 112377,  31935,  99425,   3837,\n",
            "         104543,  44793,  36987,  56568,  12210,   3837, 109978,  16530, 100921,\n",
            "           1773, 109978, 100644,  80158, 108629,  99487, 111542,  42192,  39973,\n",
            "           9370,  99235,  56568,   1773, 109978,  14777, 111488,   3837, 100090,\n",
            "          34187,  43288, 108634,  17714,  99235,   3837, 105500,  74763, 105500,\n",
            "         105382,  32945, 105407, 108226,  36587,  69249,   3837,  42192, 105378,\n",
            "          98650, 119988,  62945,   8997, 151643, 101953,  99243, 101269,  44793,\n",
            "          36987,  75061, 100203,  35568,   3837,  53222,  57218,  16530,  53222,\n",
            "           3837,  77288, 100284, 102557, 107682,  77144,  32945,  75061,  31207,\n",
            "         100956, 106052,  36987,  53222,   3837,  53222,   3837,  53222,   6313,\n",
            "          99795,  53222],\n",
            "        [ 32945, 105997,  36587,   3837, 105997,  99723,  95256, 100306,  26939,\n",
            "          44636,  99294, 100396,  95256,  24562,   3837,  99882,  44793,  36987,\n",
            "         118680,  36667,  55806,   3837,  99200, 100040,  52129,  24156,  32945,\n",
            "         116770,  14777, 119460,   3837, 100994,  26939,  75061, 105700, 111851,\n",
            "           3837, 100502, 111332,  36987, 102000,   3837,  56568, 109828,  30440,\n",
            "          52801,  81264,  75061, 105700,  44793,  36987, 104139, 101132,  81264,\n",
            "          37474,  36556, 112862,  94443,  44934,  14777,  63367,   3837, 110530,\n",
            "          42192,  22226,   3837,  36667,  27442,  15946,  34187,  99517, 102113,\n",
            "          64689,  14009,  44928,  64689, 102811,    527,   1773,  75061, 105700,\n",
            "         119238, 100992,  99287,   3837,  99364,  91676,  64272,  99805,   1773,\n",
            "          37474,  36556, 112862, 100867, 110651, 106855, 104935,  99517,   3837,\n",
            "          99436,  19403,  99851, 102838,   3837,  99882,  44793,  36987, 103924,\n",
            "         111447,   6313, 102000,   3837,  56568,  99387,  14053,  99494, 104060,\n",
            "          81264,    198, 151643, 101141,  99425, 102430,  16530, 109845,  18830,\n",
            "         101564,   3837]], device='cuda:0')}\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "{'loss': 3.6383, 'grad_norm': 2.6581621170043945, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "{'loss': 3.8242, 'grad_norm': 2.3253276348114014, 'learning_rate': 4.186046511627907e-05, 'epoch': 0.01}\n",
            "{'loss': 4.2162, 'grad_norm': 2.7193241119384766, 'learning_rate': 8.837209302325582e-05, 'epoch': 0.02}\n",
            "{'loss': 3.6783, 'grad_norm': 2.2706849575042725, 'learning_rate': 0.00013488372093023256, 'epoch': 0.04}\n",
            "{'loss': 3.7222, 'grad_norm': 2.7422127723693848, 'learning_rate': 0.0001813953488372093, 'epoch': 0.05}\n",
            "{'loss': 3.4697, 'grad_norm': 2.847121000289917, 'learning_rate': 0.00019850187265917603, 'epoch': 0.06}\n",
            "  6% 50/844 [00:31<08:10,  1.62it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 10.46it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 4.087676525115967, 'eval_accuracy': 0.3228346456692913, 'eval_runtime': 0.7416, 'eval_samples_per_second': 13.484, 'eval_steps_per_second': 5.394, 'epoch': 0.06}\n",
            "  6% 50/844 [00:32<08:10,  1.62it/s]\n",
            "100% 4/4 [00:00<00:00,  7.52it/s]\u001b[A\n",
            "{'loss': 3.53, 'grad_norm': 2.8047101497650146, 'learning_rate': 0.00019600499375780275, 'epoch': 0.07}\n",
            "{'loss': 3.7228, 'grad_norm': 2.4330861568450928, 'learning_rate': 0.00019350811485642947, 'epoch': 0.08}\n",
            "{'loss': 3.4386, 'grad_norm': 2.460286855697632, 'learning_rate': 0.00019101123595505618, 'epoch': 0.09}\n",
            "{'loss': 3.6462, 'grad_norm': 3.5086333751678467, 'learning_rate': 0.0001885143570536829, 'epoch': 0.11}\n",
            "{'loss': 3.5131, 'grad_norm': 2.745871067047119, 'learning_rate': 0.00018601747815230962, 'epoch': 0.12}\n",
            " 12% 100/844 [01:04<07:56,  1.56it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.50it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.74it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.020708084106445, 'eval_accuracy': 0.33937007874015745, 'eval_runtime': 0.7429, 'eval_samples_per_second': 13.46, 'eval_steps_per_second': 5.384, 'epoch': 0.12}\n",
            " 12% 100/844 [01:05<07:56,  1.56it/s]\n",
            "100% 4/4 [00:00<00:00,  7.29it/s]\u001b[A\n",
            "{'loss': 3.8089, 'grad_norm': 2.5750255584716797, 'learning_rate': 0.00018352059925093634, 'epoch': 0.13}\n",
            "{'loss': 3.5285, 'grad_norm': 2.4257867336273193, 'learning_rate': 0.00018102372034956306, 'epoch': 0.14}\n",
            "{'loss': 3.6529, 'grad_norm': 2.7154581546783447, 'learning_rate': 0.00017852684144818978, 'epoch': 0.15}\n",
            "{'loss': 3.4687, 'grad_norm': 2.4629571437835693, 'learning_rate': 0.0001760299625468165, 'epoch': 0.17}\n",
            "{'loss': 3.4401, 'grad_norm': 2.5173027515411377, 'learning_rate': 0.0001735330836454432, 'epoch': 0.18}\n",
            " 18% 150/844 [01:38<07:41,  1.50it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.44it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.908714771270752, 'eval_accuracy': 0.3535433070866142, 'eval_runtime': 0.7717, 'eval_samples_per_second': 12.959, 'eval_steps_per_second': 5.184, 'epoch': 0.18}\n",
            " 18% 150/844 [01:39<07:41,  1.50it/s]\n",
            "100% 4/4 [00:00<00:00,  7.01it/s]\u001b[A\n",
            "{'loss': 3.8021, 'grad_norm': 2.375220537185669, 'learning_rate': 0.0001710362047440699, 'epoch': 0.19}\n",
            "{'loss': 3.4275, 'grad_norm': 2.320068359375, 'learning_rate': 0.00016853932584269662, 'epoch': 0.2}\n",
            "{'loss': 3.3734, 'grad_norm': 2.517534017562866, 'learning_rate': 0.00016604244694132337, 'epoch': 0.21}\n",
            "{'loss': 3.5345, 'grad_norm': 2.5911762714385986, 'learning_rate': 0.00016354556803995006, 'epoch': 0.23}\n",
            "{'loss': 3.3529, 'grad_norm': 2.3658511638641357, 'learning_rate': 0.00016104868913857678, 'epoch': 0.24}\n",
            " 24% 200/844 [02:12<07:06,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.47it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.9513137340545654, 'eval_accuracy': 0.347244094488189, 'eval_runtime': 0.769, 'eval_samples_per_second': 13.004, 'eval_steps_per_second': 5.202, 'epoch': 0.24}\n",
            " 24% 200/844 [02:13<07:06,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.04it/s]\u001b[A\n",
            "{'loss': 3.4577, 'grad_norm': 2.5721142292022705, 'learning_rate': 0.00015855181023720352, 'epoch': 0.25}\n",
            "{'loss': 3.3321, 'grad_norm': 2.195251941680908, 'learning_rate': 0.00015605493133583021, 'epoch': 0.26}\n",
            "{'loss': 3.519, 'grad_norm': 2.3107333183288574, 'learning_rate': 0.00015355805243445693, 'epoch': 0.27}\n",
            "{'loss': 3.3286, 'grad_norm': 2.490304946899414, 'learning_rate': 0.00015106117353308365, 'epoch': 0.28}\n",
            "{'loss': 3.4806, 'grad_norm': 2.365910768508911, 'learning_rate': 0.00014856429463171037, 'epoch': 0.3}\n",
            " 30% 250/844 [02:47<06:31,  1.52it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.22it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.53it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.9108824729919434, 'eval_accuracy': 0.34960629921259845, 'eval_runtime': 0.759, 'eval_samples_per_second': 13.175, 'eval_steps_per_second': 5.27, 'epoch': 0.3}\n",
            " 30% 250/844 [02:47<06:31,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.11it/s]\u001b[A\n",
            "{'loss': 3.5681, 'grad_norm': 2.689840793609619, 'learning_rate': 0.0001460674157303371, 'epoch': 0.31}\n",
            "{'loss': 3.4706, 'grad_norm': 2.4494292736053467, 'learning_rate': 0.0001435705368289638, 'epoch': 0.32}\n",
            "{'loss': 3.3177, 'grad_norm': 2.52966046333313, 'learning_rate': 0.0001410736579275905, 'epoch': 0.33}\n",
            "{'loss': 3.3972, 'grad_norm': 2.673513889312744, 'learning_rate': 0.00013857677902621724, 'epoch': 0.34}\n",
            "{'loss': 3.3267, 'grad_norm': 2.2360808849334717, 'learning_rate': 0.00013607990012484396, 'epoch': 0.36}\n",
            " 36% 300/844 [03:21<06:04,  1.49it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.99it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.44it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.9944911003112793, 'eval_accuracy': 0.34330708661417325, 'eval_runtime': 0.7695, 'eval_samples_per_second': 12.995, 'eval_steps_per_second': 5.198, 'epoch': 0.36}\n",
            " 36% 300/844 [03:22<06:04,  1.49it/s]\n",
            "100% 4/4 [00:00<00:00,  7.05it/s]\u001b[A\n",
            "{'loss': 3.4601, 'grad_norm': 3.005876064300537, 'learning_rate': 0.00013358302122347065, 'epoch': 0.37}\n",
            "{'loss': 3.4419, 'grad_norm': 2.4593887329101562, 'learning_rate': 0.0001310861423220974, 'epoch': 0.38}\n",
            "{'loss': 3.4802, 'grad_norm': 2.2073450088500977, 'learning_rate': 0.00012858926342072412, 'epoch': 0.39}\n",
            "{'loss': 3.2053, 'grad_norm': 2.2426116466522217, 'learning_rate': 0.0001260923845193508, 'epoch': 0.4}\n",
            "{'loss': 3.3708, 'grad_norm': 2.062997341156006, 'learning_rate': 0.00012359550561797752, 'epoch': 0.41}\n",
            " 41% 350/844 [03:55<05:29,  1.50it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.22it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.50it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.961153745651245, 'eval_accuracy': 0.35118110236220473, 'eval_runtime': 0.7768, 'eval_samples_per_second': 12.873, 'eval_steps_per_second': 5.149, 'epoch': 0.41}\n",
            " 41% 350/844 [03:56<05:29,  1.50it/s]\n",
            "100% 4/4 [00:00<00:00,  7.06it/s]\u001b[A\n",
            "{'loss': 3.4429, 'grad_norm': 2.422131061553955, 'learning_rate': 0.00012109862671660426, 'epoch': 0.43}\n",
            "{'loss': 3.2546, 'grad_norm': 2.626202344894409, 'learning_rate': 0.00011860174781523096, 'epoch': 0.44}\n",
            "{'loss': 3.2293, 'grad_norm': 2.43892765045166, 'learning_rate': 0.00011610486891385768, 'epoch': 0.45}\n",
            "{'loss': 3.4961, 'grad_norm': 2.6027607917785645, 'learning_rate': 0.00011360799001248441, 'epoch': 0.46}\n",
            "{'loss': 3.4972, 'grad_norm': 2.6272499561309814, 'learning_rate': 0.00011111111111111112, 'epoch': 0.47}\n",
            " 47% 400/844 [04:29<04:51,  1.52it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.24it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.000417709350586, 'eval_accuracy': 0.347244094488189, 'eval_runtime': 0.7628, 'eval_samples_per_second': 13.109, 'eval_steps_per_second': 5.244, 'epoch': 0.47}\n",
            " 47% 400/844 [04:30<04:51,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "{'loss': 3.5708, 'grad_norm': 2.707822799682617, 'learning_rate': 0.00010861423220973783, 'epoch': 0.49}\n",
            "{'loss': 3.4239, 'grad_norm': 2.8059229850769043, 'learning_rate': 0.00010611735330836454, 'epoch': 0.5}\n",
            "{'loss': 3.5395, 'grad_norm': 2.9885661602020264, 'learning_rate': 0.00010362047440699127, 'epoch': 0.51}\n",
            "{'loss': 3.6325, 'grad_norm': 2.6548306941986084, 'learning_rate': 0.00010112359550561799, 'epoch': 0.52}\n",
            "{'loss': 3.2996, 'grad_norm': 2.404179096221924, 'learning_rate': 9.86267166042447e-05, 'epoch': 0.53}\n",
            " 53% 450/844 [05:04<04:20,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.03it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.47it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.894440174102783, 'eval_accuracy': 0.35748031496062993, 'eval_runtime': 0.7654, 'eval_samples_per_second': 13.065, 'eval_steps_per_second': 5.226, 'epoch': 0.53}\n",
            " 53% 450/844 [05:04<04:20,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.06it/s]\u001b[A\n",
            "{'loss': 3.4631, 'grad_norm': 2.489227771759033, 'learning_rate': 9.612983770287141e-05, 'epoch': 0.55}\n",
            "{'loss': 3.393, 'grad_norm': 2.503164291381836, 'learning_rate': 9.363295880149813e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2633, 'grad_norm': 2.693021297454834, 'learning_rate': 9.113607990012485e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3387, 'grad_norm': 2.559497833251953, 'learning_rate': 8.863920099875155e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4189, 'grad_norm': 2.5726053714752197, 'learning_rate': 8.614232209737829e-05, 'epoch': 0.59}\n",
            " 59% 500/844 [05:38<03:48,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.99it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.8559353351593018, 'eval_accuracy': 0.36141732283464567, 'eval_runtime': 0.7704, 'eval_samples_per_second': 12.981, 'eval_steps_per_second': 5.192, 'epoch': 0.59}\n",
            " 59% 500/844 [05:39<03:48,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.01it/s]\u001b[A\n",
            "{'loss': 3.3243, 'grad_norm': 2.514737844467163, 'learning_rate': 8.3645443196005e-05, 'epoch': 0.6}\n",
            "{'loss': 3.4953, 'grad_norm': 2.4885993003845215, 'learning_rate': 8.114856429463171e-05, 'epoch': 0.62}\n",
            "{'loss': 3.3347, 'grad_norm': 2.5169525146484375, 'learning_rate': 7.865168539325843e-05, 'epoch': 0.63}\n",
            "{'loss': 3.5258, 'grad_norm': 2.2446036338806152, 'learning_rate': 7.615480649188515e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3043, 'grad_norm': 2.3993618488311768, 'learning_rate': 7.365792759051186e-05, 'epoch': 0.65}\n",
            " 65% 550/844 [06:12<03:14,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.26it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.56it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.855503559112549, 'eval_accuracy': 0.35669291338582676, 'eval_runtime': 0.7609, 'eval_samples_per_second': 13.142, 'eval_steps_per_second': 5.257, 'epoch': 0.65}\n",
            " 65% 550/844 [06:13<03:14,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.11it/s]\u001b[A\n",
            "{'loss': 3.3172, 'grad_norm': 2.3205177783966064, 'learning_rate': 7.116104868913858e-05, 'epoch': 0.66}\n",
            "{'loss': 3.38, 'grad_norm': 2.5923898220062256, 'learning_rate': 6.86641697877653e-05, 'epoch': 0.68}\n",
            "{'loss': 3.3734, 'grad_norm': 2.5266330242156982, 'learning_rate': 6.6167290886392e-05, 'epoch': 0.69}\n",
            "{'loss': 3.2275, 'grad_norm': 2.7929084300994873, 'learning_rate': 6.367041198501872e-05, 'epoch': 0.7}\n",
            "{'loss': 3.394, 'grad_norm': 2.5463998317718506, 'learning_rate': 6.117353308364546e-05, 'epoch': 0.71}\n",
            " 71% 600/844 [06:47<02:42,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.53it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.8253440856933594, 'eval_accuracy': 0.35826771653543305, 'eval_runtime': 0.7648, 'eval_samples_per_second': 13.075, 'eval_steps_per_second': 5.23, 'epoch': 0.71}\n",
            " 71% 600/844 [06:47<02:42,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.06it/s]\u001b[A\n",
            "{'loss': 3.2638, 'grad_norm': 2.1853549480438232, 'learning_rate': 5.867665418227216e-05, 'epoch': 0.72}\n",
            "{'loss': 3.2648, 'grad_norm': 3.148205280303955, 'learning_rate': 5.6179775280898885e-05, 'epoch': 0.73}\n",
            "{'loss': 3.3278, 'grad_norm': 2.604661703109741, 'learning_rate': 5.368289637952559e-05, 'epoch': 0.75}\n",
            "{'loss': 3.4756, 'grad_norm': 2.6150007247924805, 'learning_rate': 5.1186017478152315e-05, 'epoch': 0.76}\n",
            "{'loss': 3.1948, 'grad_norm': 2.4900829792022705, 'learning_rate': 4.8689138576779034e-05, 'epoch': 0.77}\n",
            " 77% 650/844 [07:21<02:09,  1.50it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.49it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.8345425128936768, 'eval_accuracy': 0.3598425196850394, 'eval_runtime': 0.767, 'eval_samples_per_second': 13.037, 'eval_steps_per_second': 5.215, 'epoch': 0.77}\n",
            " 77% 650/844 [07:22<02:09,  1.50it/s]\n",
            "100% 4/4 [00:00<00:00,  7.08it/s]\u001b[A\n",
            "{'loss': 3.3656, 'grad_norm': 2.489828586578369, 'learning_rate': 4.6192259675405745e-05, 'epoch': 0.78}\n",
            "{'loss': 3.4311, 'grad_norm': 2.349761724472046, 'learning_rate': 4.3695380774032463e-05, 'epoch': 0.79}\n",
            "{'loss': 3.3225, 'grad_norm': 2.2325997352600098, 'learning_rate': 4.119850187265918e-05, 'epoch': 0.81}\n",
            "{'loss': 3.4163, 'grad_norm': 2.456137180328369, 'learning_rate': 3.870162297128589e-05, 'epoch': 0.82}\n",
            "{'loss': 3.2606, 'grad_norm': 2.570406198501587, 'learning_rate': 3.620474406991261e-05, 'epoch': 0.83}\n",
            " 83% 700/844 [07:55<01:35,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.18it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.53it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.83838152885437, 'eval_accuracy': 0.36141732283464567, 'eval_runtime': 0.7626, 'eval_samples_per_second': 13.113, 'eval_steps_per_second': 5.245, 'epoch': 0.83}\n",
            " 83% 700/844 [07:56<01:35,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.08it/s]\u001b[A\n",
            "{'loss': 3.4016, 'grad_norm': 2.3048946857452393, 'learning_rate': 3.370786516853933e-05, 'epoch': 0.84}\n",
            "{'loss': 3.2623, 'grad_norm': 2.308324098587036, 'learning_rate': 3.121098626716604e-05, 'epoch': 0.85}\n",
            "{'loss': 3.2407, 'grad_norm': 2.168170928955078, 'learning_rate': 2.871410736579276e-05, 'epoch': 0.86}\n",
            "{'loss': 3.3768, 'grad_norm': 2.1220619678497314, 'learning_rate': 2.6217228464419475e-05, 'epoch': 0.88}\n",
            "{'loss': 3.1661, 'grad_norm': 2.529977321624756, 'learning_rate': 2.3720349563046193e-05, 'epoch': 0.89}\n",
            " 89% 750/844 [08:29<01:02,  1.51it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.38it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.60it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.8457751274108887, 'eval_accuracy': 0.3590551181102362, 'eval_runtime': 0.7581, 'eval_samples_per_second': 13.19, 'eval_steps_per_second': 5.276, 'epoch': 0.89}\n",
            " 89% 750/844 [08:30<01:02,  1.51it/s]\n",
            "100% 4/4 [00:00<00:00,  7.10it/s]\u001b[A\n",
            "{'loss': 3.3661, 'grad_norm': 2.5542328357696533, 'learning_rate': 2.1223470661672908e-05, 'epoch': 0.9}\n",
            "{'loss': 3.3815, 'grad_norm': 2.377031087875366, 'learning_rate': 1.8726591760299626e-05, 'epoch': 0.91}\n",
            "{'loss': 3.386, 'grad_norm': 2.6321229934692383, 'learning_rate': 1.6229712858926345e-05, 'epoch': 0.92}\n",
            "{'loss': 3.2096, 'grad_norm': 2.4041264057159424, 'learning_rate': 1.373283395755306e-05, 'epoch': 0.94}\n",
            "{'loss': 3.3889, 'grad_norm': 2.7234394550323486, 'learning_rate': 1.1235955056179776e-05, 'epoch': 0.95}\n",
            " 95% 800/844 [09:04<00:29,  1.49it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.06it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.48it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.842071533203125, 'eval_accuracy': 0.36299212598425196, 'eval_runtime': 0.769, 'eval_samples_per_second': 13.003, 'eval_steps_per_second': 5.201, 'epoch': 0.95}\n",
            " 95% 800/844 [09:04<00:29,  1.49it/s]\n",
            "100% 4/4 [00:00<00:00,  7.08it/s]\u001b[A\n",
            "{'loss': 3.2271, 'grad_norm': 2.305424213409424, 'learning_rate': 8.739076154806493e-06, 'epoch': 0.96}\n",
            "{'loss': 3.2425, 'grad_norm': 2.3965933322906494, 'learning_rate': 6.2421972534332085e-06, 'epoch': 0.97}\n",
            "{'loss': 3.3951, 'grad_norm': 2.5305819511413574, 'learning_rate': 3.7453183520599255e-06, 'epoch': 0.98}\n",
            "{'loss': 3.301, 'grad_norm': 2.1735353469848633, 'learning_rate': 1.2484394506866417e-06, 'epoch': 1.0}\n",
            "{'train_runtime': 574.9772, 'train_samples_per_second': 4.404, 'train_steps_per_second': 1.468, 'train_loss': 3.4272792093561724, 'epoch': 1.0}\n",
            "100% 844/844 [09:34<00:00,  1.47it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   656130GF\n",
            "  train_loss               =     3.4273\n",
            "  train_runtime            = 0:09:34.97\n",
            "  train_samples            =       2532\n",
            "  train_samples_per_second =      4.404\n",
            "  train_steps_per_second   =      1.468\n",
            "\u001b[32m2026-01-23 05:38:15.752\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m756\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 574.9772, 'train_samples_per_second': 4.404, 'train_steps_per_second': 1.468, 'total_flos': 704514471100416.0, 'train_loss': 3.4272792093561724, 'epoch': 1.0, 'train_samples': 2532}\u001b[0m\n",
            "\u001b[32m2026-01-23 05:38:15.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m757\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2026-01-23 05:38:16.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m765\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 4/4 [00:00<00:00,  7.27it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.3583\n",
            "  eval_loss               =     3.8423\n",
            "  eval_runtime            = 0:00:00.75\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     13.198\n",
            "  eval_steps_per_second   =      5.279\n",
            "  perplexity              =    46.6326\n",
            "\u001b[32m2026-01-23 05:38:17.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m778\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.8423008918762207, 'eval_accuracy': 0.35826771653543305, 'eval_runtime': 0.7577, 'eval_samples_per_second': 13.198, 'eval_steps_per_second': 5.279, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 46.63264777581191}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjF6lD_0i0BZ",
        "outputId": "1fa25195-ee59-48aa-a03b-b4d7660fd055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root 1.1K Jan 23 05:38 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Jan 23 05:38 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Jan 23 05:38 added_tokens.json\n",
            "-rw-r--r-- 1 root root  472 Jan 23 05:38 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 05:38 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Jan 23 05:37 \u001b[0m\u001b[01;34mcheckpoint-750\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Jan 23 05:37 \u001b[01;34mcheckpoint-800\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Jan 23 05:38 \u001b[01;34mcheckpoint-844\u001b[0m/\n",
            "-rw-r--r-- 1 root root  263 Jan 23 05:38 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Jan 23 05:38 merges.txt\n",
            "-rw-r--r-- 1 root root 5.1K Jan 23 05:38 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Jan 23 05:28 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  616 Jan 23 05:38 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.7K Jan 23 05:38 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  20K Jan 23 05:38 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Jan 23 05:38 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Jan 23 05:38 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCseY0Ysi0BZ"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pWLo6sBvi0BZ"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU3zUMZMi0Ba",
        "outputId": "ea6025a9-53c0-4ec8-81ef-a0a6f516b5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 05:40:41.079561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769146841.100656    6979 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769146841.106963    6979 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769146841.122868    6979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146841.122893    6979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146841.122897    6979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146841.122902    6979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 05:40:41.128919: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: Qwen/Qwen2.5-0.5B\n",
            "LoRA model: outputs-pt-v1\n",
            "Loading LoRA for causal language model\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-pt/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwXbuZ66i0Ba",
        "outputId": "cdbad2bf-1b02-4a71-bbc4-dfb66f9c3cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Jan 23 05:40 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 05:40 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Jan 23 05:40 config.json\n",
            "-rw-r--r-- 1 root root  117 Jan 23 05:40 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Jan 23 05:40 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Jan 23 05:40 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Jan 23 05:40 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Jan 23 05:40 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Jan 23 05:40 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Jan 23 05:40 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfqCDpri0Ba",
        "outputId": "d9de34fb-51ca-4da8-d674-afc7a3035701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.6\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxMKaoShi0Ba"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wRT-Y3Z8i0Ba"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "VzubSQ2pi0Ba"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JpkW6cnoi0Ba"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJLYAPDEi0Ba",
        "outputId": "21a5aee2-a6c6-4697-a944-8679a56b32af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical_sft_1K_format.jsonl        sharegpt_zh_1K_format.jsonl\n",
            "numina_cot_sharegpt_data_1k.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('./data/finetune/numina_cot_sharegpt_data_1k.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 3:  # 只显示前3行\n",
        "            break\n",
        "        print(f\"\\n第 {i+1} 行：\")\n",
        "        try:\n",
        "            data = json.loads(line.strip())\n",
        "            print(json.dumps(data, indent=2, ensure_ascii=False))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON解析错误: {e}\")\n",
        "            print(f\"原始行内容: {line.strip()}\")\n",
        "\n",
        "with open('./data/finetune/medical_sft_1K_format.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 3:  # 只显示前3行\n",
        "            break\n",
        "        print(f\"\\n第 {i+1} 行：\")\n",
        "        try:\n",
        "            data = json.loads(line.strip())\n",
        "            print(json.dumps(data, indent=2, ensure_ascii=False))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON解析错误: {e}\")\n",
        "            print(f\"原始行内容: {line.strip()}\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4IOahkozQZ8",
        "outputId": "aba86ff2-eb1e-43e2-d8c7-79c64be57a21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "第 1 行：\n",
            "{\n",
            "  \"id\": \"AI-MO/NuminaMath-CoT-0\",\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"Consider the terms of an arithmetic sequence: $-\\\\frac{1}{3}, y+2, 4y, \\\\ldots$. Solve for $y$. Let's think step by step and output the final answer within \\\\boxed{}.\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"For an arithmetic sequence, the difference between consecutive terms must be equal. Therefore, we can set up the following equations based on the sequence given:\\n\\\\[ (y + 2) - \\\\left(-\\\\frac{1}{3}\\\\right) = 4y - (y+2) \\\\]\\n\\nSimplify and solve these equations:\\n\\\\[ y + 2 + \\\\frac{1}{3} = 4y - y - 2 \\\\]\\n\\\\[ y + \\\\frac{7}{3} = 3y - 2 \\\\]\\n\\\\[ \\\\frac{7}{3} + 2 = 3y - y \\\\]\\n\\\\[ \\\\frac{13}{3} = 2y \\\\]\\n\\\\[ y = \\\\frac{13}{6} \\\\]\\n\\nThus, the value of $y$ that satisfies the given arithmetic sequence is $\\\\boxed{\\\\frac{13}{6}}$.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "第 2 行：\n",
            "{\n",
            "  \"id\": \"AI-MO/NuminaMath-CoT-1\",\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"Suppose that $g(x) = 5x - 3$. What is $g^{-1}(g^{-1}(14))$? Let's think step by step and output the final answer within \\\\boxed{}.\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"First, we need to find the inverse function $g^{-1}(x)$. Given $g(x) = 5x - 3$, solve for $x$:\\n\\\\[ y = 5x - 3 \\\\]\\n\\\\[ y + 3 = 5x \\\\]\\n\\\\[ x = \\\\frac{y + 3}{5} \\\\]\\nThus, $g^{-1}(x) = \\\\frac{x + 3}{5}$.\\n\\nNow, apply $g^{-1}$ twice to the given value $14$:\\n\\\\[ g^{-1}(14) = \\\\frac{14 + 3}{5} = \\\\frac{17}{5} \\\\]\\n\\\\[ g^{-1}\\\\left(\\\\frac{17}{5}\\\\right) = \\\\frac{\\\\frac{17}{5} + 3}{5} = \\\\frac{\\\\frac{17}{5} + \\\\frac{15}{5}}{5} = \\\\frac{32}{5 \\\\times 5} = \\\\frac{32}{25} \\\\]\\n\\nThus, $g^{-1}(g^{-1}(14)) = \\\\boxed{\\\\frac{32}{25}}$.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "第 3 行：\n",
            "{\n",
            "  \"id\": \"AI-MO/NuminaMath-CoT-2\",\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"A farmer has a rectangular field with dimensions $3m+8$ and $m-3$ where $m$ is a positive integer. If the field has an area of 76 square meters, find the value of $m$. Let's think step by step and output the final answer within \\\\boxed{}.\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"Using the given dimensions, we set up the area equation:\\n\\\\[\\n(3m+8)(m-3) = 76.\\n\\\\]\\nExpanding this, we get:\\n\\\\[\\n3m^2 - 9m + 8m - 24 = 76,\\n\\\\]\\n\\\\[\\n3m^2 - m - 24 = 76,\\n\\\\]\\n\\\\[\\n3m^2 - m - 100 = 0.\\n\\\\]\\nFactoring the quadratic, we find:\\n\\\\[\\n(3m+25)(m-4) = 0.\\n\\\\]\\nThis gives two potential solutions for $m$: $m=-\\\\frac{25}{3}$ and $m=4$. Since $m$ must be a positive integer, the only valid solution is $m = \\\\boxed{4}$.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "第 1 行：\n",
            "{\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "第 2 行：\n",
            "{\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"两只脚明显大小不一样，腿也不一样粗，该怎么办，两只脚明显大小不一样，腿也不一样粗，该怎么办，需要做什么检查\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"，与走路姿势没有关系的，人的器官，没有完全对称的，只是有的不是很明显的，这很正常的，只要健康就好。只有手术能纠正的。\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "第 3 行：\n",
            "{\n",
            "  \"conversations\": [\n",
            "    {\n",
            "      \"from\": \"human\",\n",
            "      \"value\": \"精子生成减少的病因是什么？\"\n",
            "    },\n",
            "    {\n",
            "      \"from\": \"gpt\",\n",
            "      \"value\": \"支持细胞错位\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 1. 定义文件路径\n",
        "problem_file = './data/finetune/numina_cot_sharegpt_data_1k.jsonl'\n",
        "\n",
        "# 2. 读取、处理并写回（覆盖原文件，建议先备份）\n",
        "fixed_lines = []\n",
        "with open(problem_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line.strip())\n",
        "        # 移除'id'字段\n",
        "        if 'id' in data:\n",
        "            del data['id']\n",
        "        # 确保格式正确，只保留 conversations\n",
        "        fixed_lines.append(json.dumps({'conversations': data['conversations']}, ensure_ascii=False))\n",
        "\n",
        "with open(problem_file, 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(fixed_lines))\n",
        "\n",
        "print(f\"文件 ‘{problem_file}’ 已修复，多余的 ‘id’ 字段已移除。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHI__RaU2R2M",
        "outputId": "b26b9d0a-fb44-4f30-b5cf-55b751cac905"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件 ‘./data/finetune/numina_cot_sharegpt_data_1k.jsonl’ 已修复，多余的 ‘id’ 字段已移除。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7e9RynZi0Bc",
        "outputId": "81fdcfd7-266a-4c28-bbcf-5a8b064d6006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 05:41:05.860097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769146865.879676    7096 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769146865.885547    7096 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769146865.899961    7096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146865.899985    7096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146865.899989    7096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769146865.899992    7096 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 05:41:05.904346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-01-23 05:41:11.940\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m192\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m348\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=no,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-sft-v1/runs/Jan23_05-41-11_e9f480ff6040,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs=None,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-sft-v1,\n",
            "overwrite_output_dir=True,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "project=huggingface,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "trackio_space_id=trackio,\n",
            "use_cpu=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='merged-pt', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mtrain files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/numina_cot_sharegpt_data_1k.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m416\u001b[0m - \u001b[1meval files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/numina_cot_sharegpt_data_1k.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
            "Generating train split: 3000 examples [00:00, 70122.84 examples/s]\n",
            "Generating validation split: 3000 examples [00:00, 92960.93 examples/s]\n",
            "\u001b[32m2026-01-23 05:41:12.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:12.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m534\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '聚醚醚酮 (jú è èr jīn)'}, {'from': 'gpt', 'value': '聚醚酰亚胺（PEEK）是一种高性能的工程热塑性塑料，以其卓越的力学、热学和化学性能而著称。PEEK是一种半结晶聚合物，其高强度、刚度和韧性以及出色的耐热、耐化学品和耐磨性为其特征。\\nPEEK被广泛应用于各个行业，包括航空航天、汽车、医疗和石油天然气。它被用于需要高性能材料的应用中，如结构组件、轴承、密封件和电气绝缘件。\\nPEEK的独特性质之一是其耐高温性能。它的熔点约为343℃(649℉)，并且可以在长时间内在高达250℃(482℉)的温度下使用而不失去其力学性能。\\nPEEK也高度耐化学性，可以承受恶劣环境的暴露而不降解。它对大多数有机溶剂、酸和碱都具有抵抗能力，并可用于其他塑料失败的应用。\\n总之，PEEK是一种多用途的材料，具有出色的力学、热学和化学性能，使其成为广泛高性能应用的理想选择。'}, {'from': 'human', 'value': '应用于膜制造'}, {'from': 'gpt', 'value': 'PEEK由于其独特的特性还被用于膜制造。由PEEK制成的膜具有极佳的化学稳定性和耐久性，适用于各种应用，包括水处理、气体分离和生物医学应用。\\n在水处理中，PEEK膜用于从水中过滤出杂质，包括细菌和病毒。这些膜在去除杂质方面非常有效，可用于各种水处理应用，包括海水淡化、废水处理和饮用水净化。\\n在气体分离中，PEEK膜用于根据分子大小和形状分离气体。这些膜在气体分离方面非常有效，可用于各种应用，包括空气分离、天然气加工和氢气纯化。\\n在生物医学应用中，PEEK膜用作可植入设备，包括心血管植入物、整形外科植入物和药物输送系统。PEEK是一种生物相容材料，被人体良好耐受且不会产生不良反应或排斥。\\n总的来说，PEEK是一种多用途的材料，由于其极佳的化学稳定性，耐久性和生物相容性，非常适合用于膜制造。'}, {'from': 'human', 'value': 'PEEK用于膜制造所需特性。'}, {'from': 'gpt', 'value': 'PEEK 具有以下几种特性，使其成为制造膜材料的理想材料：\\n1. 化学稳定性：PEEK 具有优异的耐化学性能，包括对酸、碱和有机溶剂的良好耐受性。这种特性使其非常适合在膜可能与强化学物质接触的应用中使用。\\n2. 热稳定性：PEEK 可以在高温下保持高度稳定，因此非常适合在使用、清洁或灭菌过程中，膜需要暴露于高温的应用中使用。\\n3. 机械强度：PEEK 是一种高耐久、具有优异机械性能的材料，包括高抗拉强度、刚度和韧性。这使其非常适合在需要受到机械应力或变形的应用中使用。\\n4. 生物相容性：PEEK 是一种生物相容性材料，人体能够很好地耐受它，不会引起不良反应或排斥反应。这使其非常适合在包括可植入设备在内的生物医学应用中使用。\\n5. 疏水性：PEEK 是一种疏水性材料，不易吸收水和其他液体。这使其非常适合在需要膜保持干燥或在液体存在的情况下保持其结构的应用中使用。\\n综上所述，这些特性的结合使得 PEEK 成为制造膜的理想材料，尤其是在需要耐久性、化学稳定性和热稳定性的应用中。'}, {'from': 'human', 'value': '使用PEEK的膜制造过程'}, {'from': 'gpt', 'value': '有几个使用PEEK的膜制造过程，包括：\\n1. 相转变：这是一种广泛使用的膜制造过程，涉及形成聚合物溶液，然后将其浇铸或挤出成平板或管状膜形。然后将膜浸泡在非溶剂浴中诱导相分离，从而形成多孔结构。可以通过在适当的溶剂中溶解PEEK，然后将其浇铸或挤出成膜形。\\n2. 电纺：这个过程涉及使用静电场将聚合物溶液拉成纳米纤维，从而形成高度多孔的膜结构。可以通过在适当的溶剂中溶解PEEK，然后进行电纺成纳米纤维膜。\\n3. 热诱导相分离：这个过程涉及形成聚合物溶液，然后加热以诱导相分离并创建多孔膜结构。可以通过在适当的溶剂中溶解PEEK，然后加热以诱导相分离。\\n4. 聚合物混合：这个过程涉及将PEEK与其他聚合物混合，以创建具有特定性能的膜。例如，将PEEK与聚偏氟乙烯（PVDF）混合可以创建具有增强机械强度和化学耐受性的膜。\\n总的来说，PEEK可以用于各种膜制造过程，以创建具有特定性能，包括孔隙率，机械强度和化学耐受性的膜。所使用的具体制造过程取决于膜的所需特性和预期应用。'}, {'from': 'human', 'value': '聚醚酮(PEEK) 平均分子量'}, {'from': 'gpt', 'value': 'PEEK（聚醚醚酮）的平均分子量取决于制造工艺和预期的应用。通常，PEEK的平均分子量在10000至100000克/摩尔之间变化。一般来说，较高分子量的PEEK具有更好的机械性能，包括更高的刚度和强度，而较低分子量的PEEK具有更好的可加工性，包括更容易的熔融流动和更高的溶解度。通过调整反应条件和反应物使用量，可以控制PEEK的分子量。通过使用更少反应物或更短的反应时间可以生产出较低分子量的PEEK，而通过使用更多反应物或更长的反应时间可以生产出较高分子量的PEEK。总的来说，PEEK的平均分子量取决于预期的应用，通常情况下，在高性能应用中使用较高分子量的PEEK，如航空航天和汽车，而在注塑和挤出等加工应用中使用较低分子量的PEEK。'}, {'from': 'human', 'value': 'PEEK在气体分离膜应用中的全球和印度制造商前20名'}, {'from': 'gpt', 'value': '以下是全球和印度PEEK气体分离膜应用的顶级20家制造商：\\n\\n全球制造商：\\n1. Victrex plc\\n2. Solvay S.A.\\n3. Evonik Industries AG\\n4. Arkema S.A.\\n5. Celanese Corporation\\n6. BASF SE\\n7. Quadrant AG\\n8. RTP Company\\n9. Ensinger GmbH\\n10. SABIC\\n\\n印度制造商：\\n1. Gharda Chemicals Ltd.\\n2. Zeus Industrial Products, Inc.\\n3. Aristo Biotech and Life Science Pvt. Ltd.\\n4. Plastena India Pvt. Ltd.\\n5. Polyplastics Co. Ltd.\\n6. J. K. Overseas\\n7. Vin Industries\\n8. Maha Chemicals Asia Pte. Ltd.\\n9. Ketan Engineering Co.\\n10. Jyoti Polymer\\n\\n值得注意的是，尽管这些公司生产PEEK，但并不是所有公司都专门生产用于气体分离膜应用的PEEK。一些公司可能会拥有更广泛的PEEK产品范围，用于不同的行业和应用。'}]}\u001b[0m\n",
            "Running tokenizer on dataset: 100% 1000/1000 [00:10<00:00, 98.46 examples/s]\n",
            "Filter: 100% 997/997 [00:00<00:00, 2429.07 examples/s]\n",
            "\u001b[32m2026-01-23 05:41:25.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m551\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 997\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:25.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:25.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m553\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 聚醚醚酮 (jú è èr jīn) ASSISTANT:聚醚酰亚胺（PEEK）是一种高性能的工程热塑性塑料，以其卓越的力学、热学和化学性能而著称。PEEK是一种半结晶聚合物，其高强度、刚度和韧性以及出色的耐热、耐化学品和耐磨性为其特征。\n",
            "PEEK被广泛应用于各个行业，包括航空航天、汽车、医疗和石油天然气。它被用于需要高性能材料的应用中，如结构组件、轴承、密封件和电气绝缘件。\n",
            "PEEK的独特性质之一是其耐高温性能。它的熔点约为343℃(649℉)，并且可以在长时间内在高达250℃(482℉)的温度下使用而不失去其力学性能。\n",
            "PEEK也高度耐化学性，可以承受恶劣环境的暴露而不降解。它对大多数有机溶剂、酸和碱都具有抵抗能力，并可用于其他塑料失败的应用。\n",
            "总之，PEEK是一种多用途的材料，具有出色的力学、热学和化学性能，使其成为广泛高性能应用的理想选择。<|endoftext|></s>USER: 应用于膜制造 ASSISTANT:PEEK由于其独特的特性还被用于膜制造。由PEEK制成的膜具有极佳的化学稳定性和耐久性，适用于各种应用，包括水处理、气体分离和生物医学应用。\n",
            "在水处理中，PEEK膜用于从水中过滤出杂质，包括细菌和病毒。这些膜在去除杂质方面非常有效，可用于各种水处理应用，包括海水淡化、废水处理和饮用水净化。\n",
            "在气体分离中，PEEK膜用于根据分子大小和形状分离气体。这些膜在气体分离方面非常有效，可用于各种应用，包括空气分离、天然气加工和氢气纯化。\n",
            "在生物医学应用中，PEEK膜用作可植入设备，包括心血管植入物、整形外科植入物和药物输送系统。PEEK是一种生物相容材料，被人体良好耐受且不会产生不良反应或排斥。\n",
            "总的来说，PEEK是一种多用途的材料，由于其极佳的化学稳定性，耐久性和生物相容性，非常适合用于膜制造。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:25.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>聚醚酰亚胺（PEEK）是一种高性能的工程热塑性塑料，以其卓越的力学、热学和化学性能而著称。PEEK是一种半结晶聚合物，其高强度、刚度和韧性以及出色的耐热、耐化学品和耐磨性为其特征。\n",
            "PEEK被广泛应用于各个行业，包括航空航天、汽车、医疗和石油天然气。它被用于需要高性能材料的应用中，如结构组件、轴承、密封件和电气绝缘件。\n",
            "PEEK的独特性质之一是其耐高温性能。它的熔点约为343℃(649℉)，并且可以在长时间内在高达250℃(482℉)的温度下使用而不失去其力学性能。\n",
            "PEEK也高度耐化学性，可以承受恶劣环境的暴露而不降解。它对大多数有机溶剂、酸和碱都具有抵抗能力，并可用于其他塑料失败的应用。\n",
            "总之，PEEK是一种多用途的材料，具有出色的力学、热学和化学性能，使其成为广泛高性能应用的理想选择。<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>PEEK由于其独特的特性还被用于膜制造。由PEEK制成的膜具有极佳的化学稳定性和耐久性，适用于各种应用，包括水处理、气体分离和生物医学应用。\n",
            "在水处理中，PEEK膜用于从水中过滤出杂质，包括细菌和病毒。这些膜在去除杂质方面非常有效，可用于各种水处理应用，包括海水淡化、废水处理和饮用水净化。\n",
            "在气体分离中，PEEK膜用于根据分子大小和形状分离气体。这些膜在气体分离方面非常有效，可用于各种应用，包括空气分离、天然气加工和氢气纯化。\n",
            "在生物医学应用中，PEEK膜用作可植入设备，包括心血管植入物、整形外科植入物和药物输送系统。PEEK是一种生物相容材料，被人体良好耐受且不会产生不良反应或排斥。\n",
            "总的来说，PEEK是一种多用途的材料，由于其极佳的化学稳定性，耐久性和生物相容性，非常适合用于膜制造。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:25.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:25.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？'}, {'from': 'gpt', 'value': '男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。'}]}\u001b[0m\n",
            "Running tokenizer on validation dataset: 100% 10/10 [00:00<00:00, 140.26 examples/s]\n",
            "Filter: 100% 10/10 [00:00<00:00, 1956.12 examples/s]\n",
            "\u001b[32m2026-01-23 05:41:28.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.272\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.273\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m586\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？ ASSISTANT:男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。<|endoftext|>\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1m🔧 大模型训练配置:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m706\u001b[0m - \u001b[1m  model_kwargs: {'config': Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.6\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            ", 'torch_dtype': torch.bfloat16, 'trust_remote_code': True, 'quantization_config': None, 'low_cpu_mem_usage': True, 'device_map': 'auto'}\u001b[0m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[32m2026-01-23 05:41:28.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m713\u001b[0m - \u001b[1m✅ 模型加载完成\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[1m📊 模型分布情况:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m718\u001b[0m - \u001b[1m🔧 使用HuggingFace设备映射:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m720\u001b[0m - \u001b[1m  : 0\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m728\u001b[0m - \u001b[1m📈 设备使用统计:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m730\u001b[0m - \u001b[1m  0: 1 个模块\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m751\u001b[0m - \u001b[1m💾 GPU内存使用情况:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m756\u001b[0m - \u001b[1m  GPU 0: 已分配=0.9GB, 缓存=0.9GB, 总计=14.7GB\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m798\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m813\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m822\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:28.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m823\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "\u001b[32m2026-01-23 05:41:28.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m845\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
            "/content/MedicalGPT/supervised_finetuning.py:862: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2026-01-23 05:41:29.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m874\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:29.034\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m876\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ...,    279, 151643, 151643]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 0]], device='cuda:0'), 'labels': tensor([[  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,    279, 151643,   -100]],\n",
            "       device='cuda:0')}\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:29.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m877\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
            "[tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,  46466,  37643, 104191,     16,\n",
            "            15,     15, 118047, 101064, 101923, 116502,   3837,  23031,  99794,\n",
            "        104456, 101089, 104956, 102072,   1773,  99652,  99730, 100630, 110661,\n",
            "        104509,  86119,   1773,  75882, 101039,  50511, 100630,  99950,  90885,\n",
            "        104238,   3837, 100630,  32664,     20,     15,     15,  17523, 101064,\n",
            "        106950,   9370, 110283,  33108,  86402, 101395,   9370, 101923,   1773,\n",
            "         35560,   3846,   2821,     25, 105846, 101923, 108672,  99950,  90885,\n",
            "        104238,  20412,  37029,  17177,  99371, 107898,  99950,  90885,   1773,\n",
            "         43288,  44063, 102031, 100345, 109668, 110283,  33108,  86402, 101395,\n",
            "        110276,  44063, 106950,  99492, 102239, 101970,  99371,  57191, 104553,\n",
            "          3837, 101889,  45181, 103991,  99371,  15946,  50404,  99623,  81800,\n",
            "        107789, 100651, 101923,   1773,  77557,   3837,  73670, 104210, 110283,\n",
            "          9909,  17447,  15946,  16872,   7552,  33108,  86402, 101395,   9909,\n",
            "        111610,  86402, 101395,   5373, 111610, 101917,   5373,  92894,  33447,\n",
            "         28291, 110870,   5373, 100141,   7552,  44063, 106950,  99492, 102239,\n",
            "        101970,  99371,   3837, 101889,  45181, 103991,  99371,  15946, 107898,\n",
            "         50404,     16,     15,     15,  17523, 101064, 100651, 101923,   3407,\n",
            "        101888,  18493, 101923,  15946, 107666,   9370, 104509,     16,     15,\n",
            "         18947,  86119,   3837, 101883,  87267, 105340, 100630,  28311,     16,\n",
            "            13,  87026, 100004,   9370, 104617, 104215, 102021,  94432,     17,\n",
            "            13,  87026,  64471, 105904,  47815,  99257,  11319, 105709,   3837,\n",
            "        101193,  99245,  99257,  94432,     18,     13,  87026, 100669,  38182,\n",
            "         99885, 101093,   9370,  99460, 101037,  11319, 105709,   3837,  87026,\n",
            "         60548,   9370, 103994, 102629, 102021,  94432,     19,     13,  87026,\n",
            "         64471, 109608, 103935, 100350, 100182, 102756,  47874,  94432,     20,\n",
            "            13,  87026,  18493, 101064,  33108, 103935,  15946, 104048,  99464,\n",
            "        101037,  94432,     21,     13,  87026,  64471,  18493, 101064,  15946,\n",
            "        103926,  99885, 102041,  40981,  94432,     22,     13,  87026, 107189,\n",
            "        102090,  85329,  33108,     14,  57191, 106729,  94432,     23,     13,\n",
            "         87026,  64471, 110906, 116389,   9370, 104210,  33071, 102657, 105905,\n",
            "         57191, 111427,  94432,     24,     13,  87026,  64471, 100651,  38182,\n",
            "         99885, 103935,  99877,  57191,  99600,  94432,     16,     15,     13,\n",
            "         87026, 107189,  99885,  38342, 101929, 104378,  57191, 104036,   3837,\n",
            "         99880,  18493, 101214, 103935, 101051, 100638,  26850, 108990, 106166,\n",
            "         99553, 104456, 101089, 104956,  99559,   9370, 100789, 113608,   3837,\n",
            "        100630,  41146,  99328,  99346, 101262,   5373,  99460,  33108, 100182,\n",
            "        102756, 104481, 104925,  99559,   5373, 117110,   5373, 102041,  40981,\n",
            "        101034,  18493, 103935, 101047, 100651,  26381,   1773,  67338, 104412,\n",
            "        101063, 100001, 105504, 100741, 105918,   3837,  44063, 102410, 102450,\n",
            "         20221, 104456, 101089, 104956, 106791,  99885,  16530, 105987,  57191,\n",
            "        104036,  90395, 104016, 109776, 109726, 101082, 105922, 100638,   1773,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,    220, 104973, 108598,  98237,\n",
            "         99534,   9370,     41,  43425,  25074,  68862,  28311, 100070, 107736,\n",
            "         22550,   1860,    341,  19803,  23955,    852,    543,  19803,    633,\n",
            "          2414,   1886,    543,  19803,  81502,  17706,    877,    317,  19803,\n",
            "           912,  22550,   2785,   1180,  16374,  14797,  28335,    317,  19803,\n",
            "          2647,  22550,  17706,    877,     11,  19298,  16374,  14797,  28335,\n",
            "           317,  19803,   3698,  22550,  17706,    877,    317,     92,  35560,\n",
            "          3846,   2821,     25,  34319,  46127,   5618,      9,    220,  75882,\n",
            "        107736,  99553, 104925,  33108,  40090, 104408,  20074, 104339,   8997,\n",
            "            59,   3276,    888,   3749,  19298,   1860,    341,  34319,  46127,\n",
            "          5618,    197,     10,   6567,     96,    222,  50984,  55338, 104408,\n",
            "          9370,  44177,   8997,    197,     10,    715,    197,     10,    569,\n",
            "           689,  94305,    227,  95312, 104408,  44177,  57191,  32100,  27369,\n",
            "          9370,   2582,   3030,   8997,    197,     59,   3276,  69604,   3030,\n",
            "         19542,  23955,    852,   2129,  34319,  46127,   5618,    197,     10,\n",
            "          6567,     96,    222,  50984, 105266, 104408,  44177,   8997,    197,\n",
            "            10,    715,    197,     10,    569,    689,  94305,    227,  95312,\n",
            "        105266, 104408,  44177,  57191,  32100,  27369,   9370,   2582,   3030,\n",
            "          8997,    197,     59,   3276,  69604,   3030,  19542,    633,   2414,\n",
            "          1886,   2129,  34319,  46127,   5618,    197,     10,   6567,     96,\n",
            "           222,  50984, 105146,    915,   9370, 104408,   8997,    197,     10,\n",
            "           715,    197,     10,    569,    903,    877,   8908,     99,    223,\n",
            "        116928,   9370, 104408,   9370,    915,   8997,    197,     10,    569,\n",
            "           689,  94305,    227,  95312,  34859, 104408,  57191,  32100,  64205,\n",
            "          9370,   2582,   3030,   8997,    197,     59,   3276,  69604,   3030,\n",
            "         19542,  81502,  17706,    877,   1215,  34319,  46127,   5618,    197,\n",
            "            10,  82339,  16628, 104408,   8997,    197,     10,    715,    197,\n",
            "            10,    569,    903,  28335,  94305,    227,  95312,  16628, 104408,\n",
            "         20074,   9370,  22550,  16374,  14797,  64429,    198,    197,     10,\n",
            "           569,    689,   6567,    234,    229,  19793,  19108,  57191,  22045,\n",
            "          9370,   2582,   3030,   8997,    197,     59,   3276,  69604,   3030,\n",
            "         19542,    912,  22550,   2785,   1180,  16374,  14797,  28335,   1215,\n",
            "         34319,  46127,   5618,    197,     10,  78103, 105146,    915,   9370,\n",
            "        104408,   8997,    197,     10,    715,    197,     10,    569,    903,\n",
            "           877,   8908,     99,    223,  50007,   9370, 104408,   9370,    915,\n",
            "          8997,    197,     10,    569,    903,  28335,  94305,    227,  95312,\n",
            "         50007,  20074,   9370,  22550,  16374,  14797,  64429,    198,    197,\n",
            "            10,    569,    689,   6567,    234,    229,  19793,  19108,  57191,\n",
            "         22045,   9370,   2582,   3030,   8997,    197,     59,   3276,  69604,\n",
            "          3030,  19542,   2647,  22550,  17706,    877,     11,  19298,  16374,\n",
            "         14797,  28335,   1215,  34319,  46127,   5618,    197,     10,  63073,\n",
            "        105146,    915,   9370, 104408,   8997,    197,     10,    715,    197,\n",
            "            10,    569,    903,    877,   8908,     99,    223,  28606,   9370,\n",
            "        104408,   9370,    915,   8997,    197,     10,    569,    689,   6567,\n",
            "           234,    229,  19793,  19108,  57191,  22045,   9370,   2582,   3030,\n",
            "          8997,    197,     59,   3276,  69604,   3030,  19542,   3698,  22550,\n",
            "         17706,    877,    317,    197,     92, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,    220,  14880,  11622,  13027,\n",
            "         47758,  46944, 101599,  18745,  43589,  21515,  90395, 108598,  53757,\n",
            "          5373,    285,  15124,  58143,  83873,  81454,   1773,  35560,   3846,\n",
            "          2821,     25, 103942,  73670,   6313,  99817, 104133, 105792,     63,\n",
            "          7554,     63,   9370,  21515,   3837, 102298, 101124,  39907,   5122,\n",
            "            63,  61373,     63,   3837,     63,    285,  15124,     63,  33108,\n",
            "            63,    450,   4584,     63,   1773,     63,  61373,     63,  39907,\n",
            "         44063,  46944, 102268,  42855,  26939,  82166,  31118, 100072, 101143,\n",
            "          3837,     63,    285,  15124,     63,  39907,  18493,  82166,  31118,\n",
            "         50647,  13343,  31526,     63,   2514,     63,   3837,     63,    450,\n",
            "          4584,     63,  39907,  28606,  62926,  31526,  82166,  31118, 113652,\n",
            "          9370, 102268,  28311,  13874,   3989,   1040,  18745,    510,    262,\n",
            "           707,   1304,   2327,   3804,    721,    982,    286,    656,   9615,\n",
            "           284,   4167,    262,    707,  53757,   1193,     11,   1509,    982,\n",
            "           286,    656,   9615,   2057,   5393,    340,    262,    707,    374,\n",
            "         15124,   1193,    982,    286,    470,    537,    656,   9615,    198,\n",
            "           262,    707,  83873,   1193,    982,    286,    470,    656,   9615,\n",
            "          8288,      7,     15,    340,  13874,   3989,  30534,  37029,     63,\n",
            "          7554,     63,  21515,   3837,  73670,  50377, 110581,  82166,  31118,\n",
            "         64429,  62926,  47872,  11622, 100646,  39907,   1773,  77557,  28311,\n",
            "         13874,   3989,     80,    284,  18745,    741,     80,  47468,      7,\n",
            "            16,    340,     80,  47468,      7,     17,    340,     80,  47468,\n",
            "             7,     18,    340,   1350,  10583,   2079,  15124,   2140,    220,\n",
            "           671,  70568,   3557,    198,   1350,  10583,  93789,   2140,    220,\n",
            "           671,  70568,    220,     16,    198,   1350,  10583,  93789,   2140,\n",
            "           220,    671,  70568,    220,     17,    198,   1350,  10583,  93789,\n",
            "          2140,    220,    671,  70568,    220,     18,    198,   1350,  10583,\n",
            "          2079,  15124,   2140,    220,    671,  70568,   3007,    198,  13874,\n",
            "          3989,  99880,  43288,  73670,  99663,  26939,  56568,   6313, 102056,\n",
            "        110117,  86119,  37945, 106525,   1773, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0')], \n",
            "labels:\n",
            "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100, 105846, 101923, 108672,  99950,  90885,\n",
            "        104238,  20412,  37029,  17177,  99371, 107898,  99950,  90885,   1773,\n",
            "         43288,  44063, 102031, 100345, 109668, 110283,  33108,  86402, 101395,\n",
            "        110276,  44063, 106950,  99492, 102239, 101970,  99371,  57191, 104553,\n",
            "          3837, 101889,  45181, 103991,  99371,  15946,  50404,  99623,  81800,\n",
            "        107789, 100651, 101923,   1773,  77557,   3837,  73670, 104210, 110283,\n",
            "          9909,  17447,  15946,  16872,   7552,  33108,  86402, 101395,   9909,\n",
            "        111610,  86402, 101395,   5373, 111610, 101917,   5373,  92894,  33447,\n",
            "         28291, 110870,   5373, 100141,   7552,  44063, 106950,  99492, 102239,\n",
            "        101970,  99371,   3837, 101889,  45181, 103991,  99371,  15946, 107898,\n",
            "         50404,     16,     15,     15,  17523, 101064, 100651, 101923,   3407,\n",
            "        101888,  18493, 101923,  15946, 107666,   9370, 104509,     16,     15,\n",
            "         18947,  86119,   3837, 101883,  87267, 105340, 100630,  28311,     16,\n",
            "            13,  87026, 100004,   9370, 104617, 104215, 102021,  94432,     17,\n",
            "            13,  87026,  64471, 105904,  47815,  99257,  11319, 105709,   3837,\n",
            "        101193,  99245,  99257,  94432,     18,     13,  87026, 100669,  38182,\n",
            "         99885, 101093,   9370,  99460, 101037,  11319, 105709,   3837,  87026,\n",
            "         60548,   9370, 103994, 102629, 102021,  94432,     19,     13,  87026,\n",
            "         64471, 109608, 103935, 100350, 100182, 102756,  47874,  94432,     20,\n",
            "            13,  87026,  18493, 101064,  33108, 103935,  15946, 104048,  99464,\n",
            "        101037,  94432,     21,     13,  87026,  64471,  18493, 101064,  15946,\n",
            "        103926,  99885, 102041,  40981,  94432,     22,     13,  87026, 107189,\n",
            "        102090,  85329,  33108,     14,  57191, 106729,  94432,     23,     13,\n",
            "         87026,  64471, 110906, 116389,   9370, 104210,  33071, 102657, 105905,\n",
            "         57191, 111427,  94432,     24,     13,  87026,  64471, 100651,  38182,\n",
            "         99885, 103935,  99877,  57191,  99600,  94432,     16,     15,     13,\n",
            "         87026, 107189,  99885,  38342, 101929, 104378,  57191, 104036,   3837,\n",
            "         99880,  18493, 101214, 103935, 101051, 100638,  26850, 108990, 106166,\n",
            "         99553, 104456, 101089, 104956,  99559,   9370, 100789, 113608,   3837,\n",
            "        100630,  41146,  99328,  99346, 101262,   5373,  99460,  33108, 100182,\n",
            "        102756, 104481, 104925,  99559,   5373, 117110,   5373, 102041,  40981,\n",
            "        101034,  18493, 103935, 101047, 100651,  26381,   1773,  67338, 104412,\n",
            "        101063, 100001, 105504, 100741, 105918,   3837,  44063, 102410, 102450,\n",
            "         20221, 104456, 101089, 104956, 106791,  99885,  16530, 105987,  57191,\n",
            "        104036,  90395, 104016, 109776, 109726, 101082, 105922, 100638,   1773,\n",
            "        151643,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,  34319,  46127,   5618,      9,    220,  75882,\n",
            "        107736,  99553, 104925,  33108,  40090, 104408,  20074, 104339,   8997,\n",
            "            59,   3276,    888,   3749,  19298,   1860,    341,  34319,  46127,\n",
            "          5618,    197,     10,   6567,     96,    222,  50984,  55338, 104408,\n",
            "          9370,  44177,   8997,    197,     10,    715,    197,     10,    569,\n",
            "           689,  94305,    227,  95312, 104408,  44177,  57191,  32100,  27369,\n",
            "          9370,   2582,   3030,   8997,    197,     59,   3276,  69604,   3030,\n",
            "         19542,  23955,    852,   2129,  34319,  46127,   5618,    197,     10,\n",
            "          6567,     96,    222,  50984, 105266, 104408,  44177,   8997,    197,\n",
            "            10,    715,    197,     10,    569,    689,  94305,    227,  95312,\n",
            "        105266, 104408,  44177,  57191,  32100,  27369,   9370,   2582,   3030,\n",
            "          8997,    197,     59,   3276,  69604,   3030,  19542,    633,   2414,\n",
            "          1886,   2129,  34319,  46127,   5618,    197,     10,   6567,     96,\n",
            "           222,  50984, 105146,    915,   9370, 104408,   8997,    197,     10,\n",
            "           715,    197,     10,    569,    903,    877,   8908,     99,    223,\n",
            "        116928,   9370, 104408,   9370,    915,   8997,    197,     10,    569,\n",
            "           689,  94305,    227,  95312,  34859, 104408,  57191,  32100,  64205,\n",
            "          9370,   2582,   3030,   8997,    197,     59,   3276,  69604,   3030,\n",
            "         19542,  81502,  17706,    877,   1215,  34319,  46127,   5618,    197,\n",
            "            10,  82339,  16628, 104408,   8997,    197,     10,    715,    197,\n",
            "            10,    569,    903,  28335,  94305,    227,  95312,  16628, 104408,\n",
            "         20074,   9370,  22550,  16374,  14797,  64429,    198,    197,     10,\n",
            "           569,    689,   6567,    234,    229,  19793,  19108,  57191,  22045,\n",
            "          9370,   2582,   3030,   8997,    197,     59,   3276,  69604,   3030,\n",
            "         19542,    912,  22550,   2785,   1180,  16374,  14797,  28335,   1215,\n",
            "         34319,  46127,   5618,    197,     10,  78103, 105146,    915,   9370,\n",
            "        104408,   8997,    197,     10,    715,    197,     10,    569,    903,\n",
            "           877,   8908,     99,    223,  50007,   9370, 104408,   9370,    915,\n",
            "          8997,    197,     10,    569,    903,  28335,  94305,    227,  95312,\n",
            "         50007,  20074,   9370,  22550,  16374,  14797,  64429,    198,    197,\n",
            "            10,    569,    689,   6567,    234,    229,  19793,  19108,  57191,\n",
            "         22045,   9370,   2582,   3030,   8997,    197,     59,   3276,  69604,\n",
            "          3030,  19542,   2647,  22550,  17706,    877,     11,  19298,  16374,\n",
            "         14797,  28335,   1215,  34319,  46127,   5618,    197,     10,  63073,\n",
            "        105146,    915,   9370, 104408,   8997,    197,     10,    715,    197,\n",
            "            10,    569,    903,    877,   8908,     99,    223,  28606,   9370,\n",
            "        104408,   9370,    915,   8997,    197,     10,    569,    689,   6567,\n",
            "           234,    229,  19793,  19108,  57191,  22045,   9370,   2582,   3030,\n",
            "          8997,    197,     59,   3276,  69604,   3030,  19542,   3698,  22550,\n",
            "         17706,    877,    317,    197,     92, 151643,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100, 103942,  73670,   6313,  99817, 104133, 105792,     63,\n",
            "          7554,     63,   9370,  21515,   3837, 102298, 101124,  39907,   5122,\n",
            "            63,  61373,     63,   3837,     63,    285,  15124,     63,  33108,\n",
            "            63,    450,   4584,     63,   1773,     63,  61373,     63,  39907,\n",
            "         44063,  46944, 102268,  42855,  26939,  82166,  31118, 100072, 101143,\n",
            "          3837,     63,    285,  15124,     63,  39907,  18493,  82166,  31118,\n",
            "         50647,  13343,  31526,     63,   2514,     63,   3837,     63,    450,\n",
            "          4584,     63,  39907,  28606,  62926,  31526,  82166,  31118, 113652,\n",
            "          9370, 102268,  28311,  13874,   3989,   1040,  18745,    510,    262,\n",
            "           707,   1304,   2327,   3804,    721,    982,    286,    656,   9615,\n",
            "           284,   4167,    262,    707,  53757,   1193,     11,   1509,    982,\n",
            "           286,    656,   9615,   2057,   5393,    340,    262,    707,    374,\n",
            "         15124,   1193,    982,    286,    470,    537,    656,   9615,    198,\n",
            "           262,    707,  83873,   1193,    982,    286,    470,    656,   9615,\n",
            "          8288,      7,     15,    340,  13874,   3989,  30534,  37029,     63,\n",
            "          7554,     63,  21515,   3837,  73670,  50377, 110581,  82166,  31118,\n",
            "         64429,  62926,  47872,  11622, 100646,  39907,   1773,  77557,  28311,\n",
            "         13874,   3989,     80,    284,  18745,    741,     80,  47468,      7,\n",
            "            16,    340,     80,  47468,      7,     17,    340,     80,  47468,\n",
            "             7,     18,    340,   1350,  10583,   2079,  15124,   2140,    220,\n",
            "           671,  70568,   3557,    198,   1350,  10583,  93789,   2140,    220,\n",
            "           671,  70568,    220,     16,    198,   1350,  10583,  93789,   2140,\n",
            "           220,    671,  70568,    220,     17,    198,   1350,  10583,  93789,\n",
            "          2140,    220,    671,  70568,    220,     18,    198,   1350,  10583,\n",
            "          2079,  15124,   2140,    220,    671,  70568,   3007,    198,  13874,\n",
            "          3989,  99880,  43288,  73670,  99663,  26939,  56568,   6313, 102056,\n",
            "        110117,  86119,  37945, 106525,   1773, 151643,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0')]\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:29.139\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m878\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 设计一份100受访家庭调查问卷，以了解印度农村妇女的情况。它应该包括十个最重要的问题。该计划应包括抽样策略，包括对500户家庭村庄的阶级和种姓的调查。 ASSISTANT:这项调查的良好抽样策略是使用分层随机抽样。这将涉及根据诸如阶级和种姓等因素将村庄划分为不同的层或群体，然后从每个层中选择一定数量的家庭参与调查。例如，可以基于阶级（上中下）和种姓（预定种姓、预定部落、其他后发阶层、一般）将村庄划分为不同的层，然后从每个层中随机选择100户家庭参与调查。\n",
            "\n",
            "关于在调查中提问的最重要的10个问题，一些可能的选择包括：\n",
            "1.您目前的婚姻状况是什么？\n",
            "2.您是否在家外工作？如果是，从事什么工作？\n",
            "3.您接受过任何正式的教育吗？如果是，您完成的最高学历是什么？\n",
            "4.您是否能够在社区获得医疗保健服务？\n",
            "5.您在家庭和社区中感到安全吗？\n",
            "6.您是否在家庭中拥有任何决策权？\n",
            "7.您是否有财务资源和/或信贷？\n",
            "8.您是否经历过任何形式的基于性别的暴力或歧视？\n",
            "9.您是否参与过任何社区组织或活动？\n",
            "10.您是否有任何未满足的需求或挑战，希望在您的社区得到解决？\n",
            "\n",
            "这些问题旨在提供印度农村妇女情况的广泛概述，包括其社会经济地位、教育和医疗保健方面的访问情况、安全保障、决策权以及在社区中的参与度。通过收集有关这些和其他因素的数据，将有可能识别出印度农村妇女面临的任何不平等或挑战，并制定适当的干预措施加以解决。<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
            "\u001b[32m2026-01-23 05:41:29.201\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m881\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>这项调查的良好抽样策略是使用分层随机抽样。这将涉及根据诸如阶级和种姓等因素将村庄划分为不同的层或群体，然后从每个层中选择一定数量的家庭参与调查。例如，可以基于阶级（上中下）和种姓（预定种姓、预定部落、其他后发阶层、一般）将村庄划分为不同的层，然后从每个层中随机选择100户家庭参与调查。\n",
            "\n",
            "关于在调查中提问的最重要的10个问题，一些可能的选择包括：\n",
            "1.您目前的婚姻状况是什么？\n",
            "2.您是否在家外工作？如果是，从事什么工作？\n",
            "3.您接受过任何正式的教育吗？如果是，您完成的最高学历是什么？\n",
            "4.您是否能够在社区获得医疗保健服务？\n",
            "5.您在家庭和社区中感到安全吗？\n",
            "6.您是否在家庭中拥有任何决策权？\n",
            "7.您是否有财务资源和/或信贷？\n",
            "8.您是否经历过任何形式的基于性别的暴力或歧视？\n",
            "9.您是否参与过任何社区组织或活动？\n",
            "10.您是否有任何未满足的需求或挑战，希望在您的社区得到解决？\n",
            "\n",
            "这些问题旨在提供印度农村妇女情况的广泛概述，包括其社会经济地位、教育和医疗保健方面的访问情况、安全保障、决策权以及在社区中的参与度。通过收集有关这些和其他因素的数据，将有可能识别出印度农村妇女面临的任何不平等或挑战，并制定适当的干预措施加以解决。<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 151643}.\n",
            "{'loss': 1.1567, 'grad_norm': 0.8079406023025513, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "{'loss': 2.1444, 'grad_norm': 0.9211285710334778, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.04}\n",
            "{'loss': 1.8986, 'grad_norm': 1.2674967050552368, 'learning_rate': 1.949367088607595e-05, 'epoch': 0.08}\n",
            "{'loss': 1.7171, 'grad_norm': 1.2396160364151, 'learning_rate': 1.8649789029535868e-05, 'epoch': 0.12}\n",
            "{'loss': 1.1427, 'grad_norm': 0.8649464845657349, 'learning_rate': 1.780590717299578e-05, 'epoch': 0.16}\n",
            "{'loss': 1.9172, 'grad_norm': 0.7743027806282043, 'learning_rate': 1.6962025316455696e-05, 'epoch': 0.2}\n",
            " 20% 50/250 [02:27<09:58,  2.99s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.63it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 3.1999659538269043, 'eval_runtime': 2.0802, 'eval_samples_per_second': 4.807, 'eval_steps_per_second': 1.442, 'epoch': 0.2}\n",
            " 20% 50/250 [02:29<09:58,  2.99s/it]\n",
            "100% 3/3 [00:01<00:00,  1.81it/s]\u001b[A\n",
            "{'loss': 1.4801, 'grad_norm': 1.0948708057403564, 'learning_rate': 1.6118143459915612e-05, 'epoch': 0.24}\n",
            "{'loss': 1.4859, 'grad_norm': 1.3127171993255615, 'learning_rate': 1.5274261603375528e-05, 'epoch': 0.28}\n",
            "{'loss': 1.6372, 'grad_norm': 1.0366618633270264, 'learning_rate': 1.4430379746835444e-05, 'epoch': 0.32}\n",
            "{'loss': 1.3971, 'grad_norm': 0.8939957022666931, 'learning_rate': 1.358649789029536e-05, 'epoch': 0.36}\n",
            "{'loss': 1.9742, 'grad_norm': 1.0946744680404663, 'learning_rate': 1.2742616033755275e-05, 'epoch': 0.4}\n",
            " 40% 100/250 [04:56<06:52,  2.75s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.61it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1248161792755127, 'eval_runtime': 2.094, 'eval_samples_per_second': 4.776, 'eval_steps_per_second': 1.433, 'epoch': 0.4}\n",
            " 40% 100/250 [04:58<06:52,  2.75s/it]\n",
            "100% 3/3 [00:01<00:00,  1.79it/s]\u001b[A\n",
            "{'loss': 1.4585, 'grad_norm': 1.1558586359024048, 'learning_rate': 1.189873417721519e-05, 'epoch': 0.44}\n",
            "{'loss': 1.6342, 'grad_norm': 1.5658934116363525, 'learning_rate': 1.1054852320675107e-05, 'epoch': 0.48}\n",
            "{'loss': 1.5318, 'grad_norm': 1.9897925853729248, 'learning_rate': 1.0210970464135021e-05, 'epoch': 0.52}\n",
            "{'loss': 1.4182, 'grad_norm': 1.7784610986709595, 'learning_rate': 9.367088607594937e-06, 'epoch': 0.56}\n",
            "{'loss': 1.5809, 'grad_norm': 1.0263816118240356, 'learning_rate': 8.523206751054853e-06, 'epoch': 0.6}\n",
            " 60% 150/250 [07:27<04:43,  2.83s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.61it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1085143089294434, 'eval_runtime': 2.0992, 'eval_samples_per_second': 4.764, 'eval_steps_per_second': 1.429, 'epoch': 0.6}\n",
            " 60% 150/250 [07:29<04:43,  2.83s/it]\n",
            "100% 3/3 [00:01<00:00,  1.79it/s]\u001b[A\n",
            "{'loss': 1.6749, 'grad_norm': 1.227906346321106, 'learning_rate': 7.679324894514768e-06, 'epoch': 0.64}\n",
            "{'loss': 1.5007, 'grad_norm': 0.6523005366325378, 'learning_rate': 6.835443037974684e-06, 'epoch': 0.68}\n",
            "{'loss': 1.7017, 'grad_norm': 1.0313800573349, 'learning_rate': 5.9915611814346e-06, 'epoch': 0.72}\n",
            "{'loss': 1.2833, 'grad_norm': 1.2659615278244019, 'learning_rate': 5.147679324894516e-06, 'epoch': 0.76}\n",
            "{'loss': 2.0876, 'grad_norm': 1.6876919269561768, 'learning_rate': 4.303797468354431e-06, 'epoch': 0.8}\n",
            " 80% 200/250 [09:59<02:35,  3.11s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.61it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.105222702026367, 'eval_runtime': 2.0989, 'eval_samples_per_second': 4.764, 'eval_steps_per_second': 1.429, 'epoch': 0.8}\n",
            " 80% 200/250 [10:01<02:35,  3.11s/it]\n",
            "100% 3/3 [00:01<00:00,  1.79it/s]\u001b[A\n",
            "{'loss': 1.6857, 'grad_norm': 1.054998755455017, 'learning_rate': 3.459915611814346e-06, 'epoch': 0.84}\n",
            "{'loss': 1.9714, 'grad_norm': 2.3185737133026123, 'learning_rate': 2.6160337552742622e-06, 'epoch': 0.88}\n",
            "{'loss': 1.5456, 'grad_norm': 2.1269662380218506, 'learning_rate': 1.7721518987341774e-06, 'epoch': 0.92}\n",
            "{'loss': 1.3798, 'grad_norm': 0.9108160138130188, 'learning_rate': 9.28270042194093e-07, 'epoch': 0.96}\n",
            "{'loss': 1.8444, 'grad_norm': 3.666964054107666, 'learning_rate': 8.438818565400844e-08, 'epoch': 1.0}\n",
            "100% 250/250 [12:21<00:00,  2.13s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.61it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.107264995574951, 'eval_runtime': 2.1047, 'eval_samples_per_second': 4.751, 'eval_steps_per_second': 1.425, 'epoch': 1.0}\n",
            "100% 250/250 [12:23<00:00,  2.13s/it]\n",
            "100% 3/3 [00:01<00:00,  1.80it/s]\u001b[A\n",
            "{'train_runtime': 743.4148, 'train_samples_per_second': 1.341, 'train_steps_per_second': 0.336, 'train_loss': 1.6397739024162292, 'epoch': 1.0}\n",
            "100% 250/250 [12:23<00:00,  2.97s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   961197GF\n",
            "  train_loss               =     1.6398\n",
            "  train_runtime            = 0:12:23.41\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      1.341\n",
            "  train_steps_per_second   =      0.336\n",
            "\u001b[32m2026-01-23 05:53:53.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m898\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 743.4148, 'train_samples_per_second': 1.341, 'train_steps_per_second': 0.336, 'total_flos': 1032077528408064.0, 'train_loss': 1.6397739024162292, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
            "\u001b[32m2026-01-23 05:53:53.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m899\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
            "\u001b[32m2026-01-23 05:53:53.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m908\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 3/3 [00:01<00:00,  1.68it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =     3.1085\n",
            "  eval_runtime            = 0:00:02.03\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      4.917\n",
            "  eval_steps_per_second   =      1.475\n",
            "  perplexity              =    22.3871\n",
            "\u001b[32m2026-01-23 05:53:55.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m921\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.1084837913513184, 'eval_runtime': 2.0338, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 1.475, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 22.387075178035712}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cxKpqI6i0Bc",
        "outputId": "8daa2f7a-e058-4a09-cf5b-d2ae95c1b7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root 1.1K Jan 23 05:53 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Jan 23 05:53 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Jan 23 05:53 added_tokens.json\n",
            "-rw-r--r-- 1 root root  431 Jan 23 05:53 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 05:53 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Jan 23 05:53 \u001b[0m\u001b[01;34mcheckpoint-250\u001b[0m/\n",
            "-rw-r--r-- 1 root root  221 Jan 23 05:53 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Jan 23 05:53 merges.txt\n",
            "-rw-r--r-- 1 root root 5.1K Jan 23 05:53 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Jan 23 05:41 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  648 Jan 23 05:53 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.7K Jan 23 05:53 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 6.0K Jan 23 05:53 trainer_state.json\n",
            "-rw-r--r-- 1 root root  230 Jan 23 05:53 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Jan 23 05:53 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qeXxu8qji0Bc"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "T1eX7Stgi0Bc"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmn0tbfDi0Bc",
        "outputId": "550db5f9-f361-436f-c361-3b44d2a6f42a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 05:54:03.752934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769147643.772447   10414 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769147643.778378   10414 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769147643.793344   10414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769147643.793375   10414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769147643.793379   10414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769147643.793382   10414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 05:54:03.797823: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-pt\n",
            "LoRA model: outputs-sft-v1\n",
            "Loading LoRA for causal language model\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The tokenizer you are loading from 'merged-pt' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to ./merged-sft\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSXijciSi0Bc",
        "outputId": "3ffec6d6-f98c-4fd6-d9e7-797bcdbe19cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Jan 23 05:54 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 05:54 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Jan 23 05:54 config.json\n",
            "-rw-r--r-- 1 root root  117 Jan 23 05:54 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Jan 23 05:54 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Jan 23 05:54 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Jan 23 05:54 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Jan 23 05:54 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Jan 23 05:54 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Jan 23 05:54 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmWJBF36i0Bc",
        "outputId": "1d5acbe1-e0dc-4d7e-fd22-972f1be2d6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.6\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iacV4QQKi0Bc"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mMCOHaAsi0Bc"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YKP-mzyni0Bc"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5BxmsO9bi0Bc"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thw89Dcli0Bc",
        "outputId": "556deaeb-cc5e-42e6-c1a8-8cb63c0b3d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_zh_500.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSskCV0ai0Bd",
        "outputId": "a5e608b0-25a7-4854-837d-3cf11dd0a2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 05:54:28.667425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769147668.686665   10531 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769147668.692407   10531 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769147668.706891   10531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769147668.706915   10531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769147668.706919   10531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769147668.706922   10531 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 05:54:28.711214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-01-23 05:54:36.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mParse args: ScriptArguments(model_name_or_path='./merged-sft', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward', validation_file_dir='./data/reward', template_name='qwen', per_device_train_batch_size=3, per_device_eval_batch_size=1, max_source_length=256, max_target_length=256, min_target_length=4, max_train_samples=1000, max_eval_samples=500, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_peft=True, qlora=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, peft_path=None, do_train=True, do_eval=True, learning_rate=0.0005, lr_scheduler_type='cosine', warmup_steps=100, weight_decay=0.05, optim='adamw_torch', fp16=False, bf16=True, gradient_checkpointing=True, gradient_accumulation_steps=4, save_steps=50, eval_steps=10, logging_steps=1, output_dir='outputs-dpo-v1', max_steps=100, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:36.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m218\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:36.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m225\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='./merged-sft', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:36.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mtrain files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:36.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m258\u001b[0m - \u001b[1meval files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "Generating train split: 500 examples [00:00, 16121.89 examples/s]\n",
            "Generating validation split: 500 examples [00:00, 43657.92 examples/s]\n",
            "\u001b[32m2026-01-23 05:54:36.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:36.805\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m321\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "Running tokenizer on dataset (num_proc=4): 100% 500/500 [00:00<00:00, 1002.28 examples/s]\n",
            "Filter: 100% 500/500 [00:00<00:00, 29664.79 examples/s]\n",
            "\u001b[32m2026-01-23 05:54:37.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m334\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 160\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m335\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m337\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "\n",
            "<|im_start|>user\n",
            "把“我吃了一点心”改写成疑问句。<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m338\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
            "当然，以下是几种重写句子“I ate a snack”为疑问句的方式：\n",
            "\n",
            "* 你吃了零食吗？\n",
            "* 你吃了什么？\n",
            "* 你吃了东西吗？\n",
            "* 你吃的是什么零食？\n",
            "* 你吃了什么作为零食？\n",
            "* 你吃了什么类型的零食？\n",
            "* 你喜欢你的零食吗？\n",
            "\n",
            "希望这能帮到你！\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
            "我吃了一点零食。\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.352\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m351\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "Running tokenizer on dataset (num_proc=4): 100% 500/500 [00:00<00:00, 1327.87 examples/s]\n",
            "Filter: 100% 500/500 [00:00<00:00, 37559.81 examples/s]\n",
            "\u001b[32m2026-01-23 05:54:37.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m364\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 160\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m365\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m367\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
            "你是一个非常聪明的AI助手，非常擅长按照指示行事。尽你所能地帮助。\n",
            "<|im_start|>user\n",
            "期末考试 问题1. 罗伯特·P·凯利曾任CEO的公司是在哪一年成立的？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
            "为了帮助您解答这个问题，我需要知道罗伯特·P·凯利曾担任过的公司的名称。请提供公司名称。\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
            "当然！罗伯特·P·凯利曾担任首席执行官的公司Uber成立于2009年3月28日。\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:37.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mDevice map: auto\u001b[0m\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[32m2026-01-23 05:54:38.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:38.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m449\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "Extracting prompt in train dataset: 100% 160/160 [00:00<00:00, 5204.30 examples/s]\n",
            "Applying chat template to train dataset: 100% 160/160 [00:00<00:00, 9926.17 examples/s]\n",
            "Tokenizing train dataset: 100% 160/160 [00:01<00:00, 99.57 examples/s]\n",
            "Extracting prompt in eval dataset: 100% 160/160 [00:00<00:00, 6580.01 examples/s]\n",
            "Applying chat template to eval dataset: 100% 160/160 [00:00<00:00, 9875.49 examples/s]\n",
            "Tokenizing eval dataset: 100% 160/160 [00:01<00:00, 103.53 examples/s]\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2026-01-23 05:54:52.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprint_trainable_parameters\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mtrainable params: 4399104 || all params: 498431872 || trainable%: 0.8825888244963597\u001b[0m\n",
            "\u001b[32m2026-01-23 05:54:52.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 151643}.\n",
            "{'loss': 0.6931, 'grad_norm': 6.034272193908691, 'learning_rate': 0.0, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -162.66537475585938, 'logps/rejected': -174.59756469726562, 'logits/chosen': -0.9897035360336304, 'logits/rejected': -1.101625919342041, 'epoch': 0.07}\n",
            "{'loss': 0.6931, 'grad_norm': 6.339944362640381, 'learning_rate': 5e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -223.29310607910156, 'logps/rejected': -152.3312530517578, 'logits/chosen': -0.6588242650032043, 'logits/rejected': -0.7388082146644592, 'epoch': 0.15}\n",
            "{'loss': 0.677, 'grad_norm': 7.542807579040527, 'learning_rate': 1e-05, 'rewards/chosen': 0.003890959545969963, 'rewards/rejected': -0.028809214010834694, 'rewards/accuracies': 0.8333333730697632, 'rewards/margins': 0.03270017355680466, 'logps/chosen': -229.61199951171875, 'logps/rejected': -171.58944702148438, 'logits/chosen': -1.1031067371368408, 'logits/rejected': -0.9468988180160522, 'epoch': 0.22}\n",
            "{'loss': 0.6923, 'grad_norm': 6.999260425567627, 'learning_rate': 1.5e-05, 'rewards/chosen': -0.002591928234323859, 'rewards/rejected': -0.004916828125715256, 'rewards/accuracies': 0.4166666865348816, 'rewards/margins': 0.002324898261576891, 'logps/chosen': -119.56835174560547, 'logps/rejected': -227.1815643310547, 'logits/chosen': -0.7072530388832092, 'logits/rejected': -0.731295108795166, 'epoch': 0.3}\n",
            "{'loss': 0.6914, 'grad_norm': 6.710169315338135, 'learning_rate': 2e-05, 'rewards/chosen': 0.0014080681139603257, 'rewards/rejected': -0.002493572421371937, 'rewards/accuracies': 0.4166666865348816, 'rewards/margins': 0.0039016404189169407, 'logps/chosen': -204.43356323242188, 'logps/rejected': -123.27833557128906, 'logits/chosen': -0.5539391040802002, 'logits/rejected': -0.587503969669342, 'epoch': 0.37}\n",
            "{'loss': 0.6856, 'grad_norm': 6.968717575073242, 'learning_rate': 2.5e-05, 'rewards/chosen': 0.009893830865621567, 'rewards/rejected': -0.005616172682493925, 'rewards/accuracies': 0.75, 'rewards/margins': 0.015510003082454205, 'logps/chosen': -182.6676025390625, 'logps/rejected': -187.4647216796875, 'logits/chosen': -0.8200902938842773, 'logits/rejected': -0.9901560544967651, 'epoch': 0.44}\n",
            "{'loss': 0.6922, 'grad_norm': 6.619093894958496, 'learning_rate': 3e-05, 'rewards/chosen': -0.0027703605592250824, 'rewards/rejected': -0.004826673306524754, 'rewards/accuracies': 0.5833333730697632, 'rewards/margins': 0.0020563125144690275, 'logps/chosen': -194.92141723632812, 'logps/rejected': -159.4138641357422, 'logits/chosen': -0.6513686776161194, 'logits/rejected': -0.4529742896556854, 'epoch': 0.52}\n",
            "{'loss': 0.6843, 'grad_norm': 6.581640720367432, 'learning_rate': 3.5000000000000004e-05, 'rewards/chosen': 0.011376776732504368, 'rewards/rejected': -0.0068689812906086445, 'rewards/accuracies': 0.5833333730697632, 'rewards/margins': 0.01824576035141945, 'logps/chosen': -196.95916748046875, 'logps/rejected': -180.48489379882812, 'logits/chosen': -0.7160883545875549, 'logits/rejected': -0.8731340765953064, 'epoch': 0.59}\n",
            "{'loss': 0.691, 'grad_norm': 6.174923419952393, 'learning_rate': 4e-05, 'rewards/chosen': 0.012853512540459633, 'rewards/rejected': 0.008018875494599342, 'rewards/accuracies': 0.5833333730697632, 'rewards/margins': 0.004834636114537716, 'logps/chosen': -165.9443817138672, 'logps/rejected': -193.41651916503906, 'logits/chosen': -0.9494995474815369, 'logits/rejected': -0.906688928604126, 'epoch': 0.67}\n",
            "{'loss': 0.692, 'grad_norm': 6.815185546875, 'learning_rate': 4.4999999999999996e-05, 'rewards/chosen': 0.017923977226018906, 'rewards/rejected': 0.014883995987474918, 'rewards/accuracies': 0.4166666865348816, 'rewards/margins': 0.0030399798415601254, 'logps/chosen': -255.68829345703125, 'logps/rejected': -269.15850830078125, 'logits/chosen': -0.5267902612686157, 'logits/rejected': -0.624085545539856, 'epoch': 0.74}\n",
            " 10% 10/100 [01:44<15:39, 10.44s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.82it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.77it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:53,  2.90it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.41it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.50it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.34it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.25it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.27it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.13it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.35it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.46it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.31it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.11it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.25it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.46it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.25it/s]\u001b[A\n",
            " 12% 19/160 [00:07<00:57,  2.45it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.62it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.65it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.44it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:51,  2.64it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:52,  2.60it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:52,  2.59it/s]\u001b[A\n",
            " 16% 26/160 [00:10<01:02,  2.16it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:07,  1.96it/s]\u001b[A\n",
            " 18% 28/160 [00:11<01:00,  2.19it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:53,  2.47it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:51,  2.55it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.29it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.47it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:48,  2.64it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.75it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.74it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.96it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.88it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.81it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.89it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.67it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.30it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.13it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.27it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:54,  2.11it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.08it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.44it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.52it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.44it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:49,  2.19it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:50,  2.12it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:46,  2.29it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:46,  2.26it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:43,  2.37it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.78it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.78it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.76it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:40,  2.47it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:42,  2.32it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:40,  2.42it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.40it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.58it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:40,  2.32it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.17it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.54it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.70it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.35it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.17it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.30it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.66it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.44it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.22it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.17it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.47it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.38it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.33it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:36,  2.13it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:34,  2.24it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:33,  2.25it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:34,  2.18it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.32it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.31it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.51it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.58it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.73it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.94it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.49it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.70it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.82it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.63it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.41it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.60it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.74it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.73it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.47it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.41it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.16it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.05it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.04it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:23,  2.27it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.40it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:24,  2.16it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:23,  2.18it/s]\u001b[A\n",
            " 69% 110/160 [00:46<00:25,  1.97it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.89it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.81it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  1.99it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.94it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.20it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.34it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.33it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.23it/s]\u001b[A\n",
            " 74% 119/160 [00:50<00:16,  2.44it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.22it/s]\u001b[A\n",
            " 76% 121/160 [00:51<00:17,  2.22it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.43it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.29it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:15,  2.40it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.49it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:14,  2.34it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.51it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.67it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.36it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.16it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.19it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.48it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.59it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.27it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:10,  2.23it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.22it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:09,  2.32it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.52it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.44it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.49it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.33it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.60it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.65it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.55it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.36it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.46it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.41it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.25it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.09it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.14it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.28it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.37it/s]\u001b[A\n",
            " 98% 156/160 [01:06<00:01,  2.03it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.03it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.03it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.09it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6499587297439575, 'eval_runtime': 68.1415, 'eval_samples_per_second': 2.348, 'eval_steps_per_second': 2.348, 'eval_rewards/chosen': 0.07604912668466568, 'eval_rewards/rejected': -0.014853546395897865, 'eval_rewards/accuracies': 0.824999988079071, 'eval_rewards/margins': 0.0909026712179184, 'eval_logps/chosen': -212.31396484375, 'eval_logps/rejected': -185.5698699951172, 'eval_logits/chosen': -0.743273138999939, 'eval_logits/rejected': -0.7570015788078308, 'epoch': 0.74}\n",
            " 10% 10/100 [02:53<15:39, 10.44s/it]\n",
            "100% 160/160 [01:07<00:00,  2.16it/s]\u001b[A\n",
            "{'loss': 0.6866, 'grad_norm': 7.167491436004639, 'learning_rate': 5e-05, 'rewards/chosen': 0.04993334040045738, 'rewards/rejected': 0.03566230461001396, 'rewards/accuracies': 0.5, 'rewards/margins': 0.01427103765308857, 'logps/chosen': -234.74087524414062, 'logps/rejected': -164.0419158935547, 'logits/chosen': -0.7354658842086792, 'logits/rejected': -0.8599278926849365, 'epoch': 0.81}\n",
            "{'loss': 0.6783, 'grad_norm': 6.402067184448242, 'learning_rate': 5.5e-05, 'rewards/chosen': 0.053774137049913406, 'rewards/rejected': 0.021998947486281395, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.03177519515156746, 'logps/chosen': -255.125732421875, 'logps/rejected': -164.62191772460938, 'logits/chosen': -0.8626835942268372, 'logits/rejected': -0.8808894157409668, 'epoch': 0.89}\n",
            "{'loss': 0.6638, 'grad_norm': 7.515475749969482, 'learning_rate': 6e-05, 'rewards/chosen': 0.09895669668912888, 'rewards/rejected': 0.03729426488280296, 'rewards/accuracies': 0.8333333730697632, 'rewards/margins': 0.06166243925690651, 'logps/chosen': -339.01495361328125, 'logps/rejected': -224.14877319335938, 'logits/chosen': -0.7707071304321289, 'logits/rejected': -0.7393895983695984, 'epoch': 0.96}\n",
            "{'loss': 0.6936, 'grad_norm': 11.790118217468262, 'learning_rate': 6.500000000000001e-05, 'rewards/chosen': 0.0698062852025032, 'rewards/rejected': 0.10561294853687286, 'rewards/accuracies': 0.0, 'rewards/margins': -0.03580666333436966, 'logps/chosen': -151.75985717773438, 'logps/rejected': -244.19906616210938, 'logits/chosen': -0.64693683385849, 'logits/rejected': -0.5613887906074524, 'epoch': 1.0}\n",
            "{'loss': 0.5967, 'grad_norm': 5.331535339355469, 'learning_rate': 7.000000000000001e-05, 'rewards/chosen': 0.1523718535900116, 'rewards/rejected': -0.05908941105008125, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 0.21146124601364136, 'logps/chosen': -155.10272216796875, 'logps/rejected': -200.0335693359375, 'logits/chosen': -0.6455211639404297, 'logits/rejected': -0.636629581451416, 'epoch': 1.07}\n",
            "{'loss': 0.5754, 'grad_norm': 4.700695037841797, 'learning_rate': 7.5e-05, 'rewards/chosen': 0.2748199701309204, 'rewards/rejected': 0.018011491745710373, 'rewards/accuracies': 1.0, 'rewards/margins': 0.25680848956108093, 'logps/chosen': -168.95452880859375, 'logps/rejected': -128.9139862060547, 'logits/chosen': -0.8284298181533813, 'logits/rejected': -0.6360354423522949, 'epoch': 1.15}\n",
            "{'loss': 0.5057, 'grad_norm': 5.337735652923584, 'learning_rate': 8e-05, 'rewards/chosen': 0.35290515422821045, 'rewards/rejected': -0.08845973014831543, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 0.4413648843765259, 'logps/chosen': -186.863525390625, 'logps/rejected': -201.45648193359375, 'logits/chosen': -0.9277336001396179, 'logits/rejected': -0.756456196308136, 'epoch': 1.22}\n",
            "{'loss': 0.4957, 'grad_norm': 4.851387977600098, 'learning_rate': 8.5e-05, 'rewards/chosen': 0.35311055183410645, 'rewards/rejected': -0.10726694762706757, 'rewards/accuracies': 1.0, 'rewards/margins': 0.4603775143623352, 'logps/chosen': -166.66159057617188, 'logps/rejected': -166.7323760986328, 'logits/chosen': -0.7706254720687866, 'logits/rejected': -0.6625473499298096, 'epoch': 1.3}\n",
            "{'loss': 0.4091, 'grad_norm': 4.601032733917236, 'learning_rate': 8.999999999999999e-05, 'rewards/chosen': 0.6385852098464966, 'rewards/rejected': -0.08152806758880615, 'rewards/accuracies': 1.0, 'rewards/margins': 0.720113217830658, 'logps/chosen': -243.59764099121094, 'logps/rejected': -173.08114624023438, 'logits/chosen': -0.9393854141235352, 'logits/rejected': -1.1508435010910034, 'epoch': 1.37}\n",
            "{'loss': 0.4044, 'grad_norm': 4.097352981567383, 'learning_rate': 9.5e-05, 'rewards/chosen': 0.6916253566741943, 'rewards/rejected': -0.08939720690250397, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7810225486755371, 'logps/chosen': -267.796875, 'logps/rejected': -230.62298583984375, 'logits/chosen': -0.6426705121994019, 'logits/rejected': -0.6611564755439758, 'epoch': 1.44}\n",
            " 20% 20/100 [04:36<15:20, 11.51s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.76it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.75it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.86it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.40it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.34it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.26it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.12it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:04,  2.30it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:02,  2.38it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:05,  2.23it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:11,  2.05it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:10,  2.05it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:05,  2.20it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:59,  2.42it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.22it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:58,  2.43it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.61it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.63it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.44it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.70it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.71it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.73it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.26it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:05,  2.02it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.25it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.50it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.58it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:55,  2.31it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.49it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.77it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.76it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.90it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.91it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.62it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:48,  2.48it/s]\u001b[A\n",
            " 26% 42/160 [00:17<00:47,  2.50it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:52,  2.23it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:55,  2.09it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:51,  2.23it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:54,  2.09it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.07it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.44it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.53it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.45it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.23it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:48,  2.19it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.39it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.36it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.85it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.83it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.80it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.53it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.35it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.44it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.22it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.42it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.19it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:42,  2.13it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:36,  2.47it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:34,  2.58it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:38,  2.26it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.13it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:38,  2.25it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:32,  2.62it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.42it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.21it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.16it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.45it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.41it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.40it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.20it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.33it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.32it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.23it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.36it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.35it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.54it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.59it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.74it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.95it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.52it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.76it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.87it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.66it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.45it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.57it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:23,  2.68it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:23,  2.65it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:25,  2.38it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:25,  2.32it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:28,  2.07it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.14it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.04it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.04it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.27it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.40it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.21it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.25it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.02it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.95it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:25,  1.86it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.03it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.98it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.22it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.36it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.34it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.44it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.43it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.25it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:15,  2.33it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.38it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:15,  2.24it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:14,  2.35it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:13,  2.45it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.63it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.34it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.34it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.15it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.19it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.48it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.65it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.33it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.32it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.31it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.40it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.58it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.49it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.54it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.37it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.63it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.68it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.56it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.37it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.46it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.42it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.20it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.03it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.09it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.20it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.33it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:02,  2.00it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.01it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.01it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.09it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.38500016927719116, 'eval_runtime': 68.0523, 'eval_samples_per_second': 2.351, 'eval_steps_per_second': 2.351, 'eval_rewards/chosen': 0.636050820350647, 'eval_rewards/rejected': -0.21229378879070282, 'eval_rewards/accuracies': 0.987500011920929, 'eval_rewards/margins': 0.848344624042511, 'eval_logps/chosen': -206.71395874023438, 'eval_logps/rejected': -187.54428100585938, 'eval_logits/chosen': -0.7540702819824219, 'eval_logits/rejected': -0.7660513520240784, 'epoch': 1.44}\n",
            " 20% 20/100 [05:44<15:20, 11.51s/it]\n",
            "100% 160/160 [01:07<00:00,  2.17it/s]\u001b[A\n",
            "{'loss': 0.3968, 'grad_norm': 4.542973518371582, 'learning_rate': 0.0001, 'rewards/chosen': 0.6392660737037659, 'rewards/rejected': -0.15888985991477966, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7981559038162231, 'logps/chosen': -276.5841064453125, 'logps/rejected': -184.3522491455078, 'logits/chosen': -0.7471241354942322, 'logits/rejected': -0.7666318416595459, 'epoch': 1.52}\n",
            "{'loss': 0.4041, 'grad_norm': 4.78790283203125, 'learning_rate': 0.000105, 'rewards/chosen': 0.48606163263320923, 'rewards/rejected': -0.27593734860420227, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7619990110397339, 'logps/chosen': -225.19757080078125, 'logps/rejected': -242.80084228515625, 'logits/chosen': -0.6080131530761719, 'logits/rejected': -0.607356607913971, 'epoch': 1.59}\n",
            "{'loss': 0.3679, 'grad_norm': 4.239808559417725, 'learning_rate': 0.00011, 'rewards/chosen': 0.7809396982192993, 'rewards/rejected': -0.19922441244125366, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9801641702651978, 'logps/chosen': -221.7630615234375, 'logps/rejected': -236.52171325683594, 'logits/chosen': -0.9815004467964172, 'logits/rejected': -1.1136595010757446, 'epoch': 1.67}\n",
            "{'loss': 0.4153, 'grad_norm': 4.038856029510498, 'learning_rate': 0.000115, 'rewards/chosen': 0.5362256765365601, 'rewards/rejected': -0.27085238695144653, 'rewards/accuracies': 0.8333333730697632, 'rewards/margins': 0.8070781230926514, 'logps/chosen': -142.0619659423828, 'logps/rejected': -141.61941528320312, 'logits/chosen': -0.5748212337493896, 'logits/rejected': -0.553007960319519, 'epoch': 1.74}\n",
            "{'loss': 0.3591, 'grad_norm': 3.835080146789551, 'learning_rate': 0.00012, 'rewards/chosen': 0.6248050332069397, 'rewards/rejected': -0.3062931299209595, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9310981035232544, 'logps/chosen': -202.5284881591797, 'logps/rejected': -174.40362548828125, 'logits/chosen': -0.7261295318603516, 'logits/rejected': -0.8697375059127808, 'epoch': 1.81}\n",
            "{'loss': 0.3261, 'grad_norm': 4.161769866943359, 'learning_rate': 0.000125, 'rewards/chosen': 0.9111967086791992, 'rewards/rejected': -0.24834196269512177, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 1.159538745880127, 'logps/chosen': -260.107177734375, 'logps/rejected': -228.95223999023438, 'logits/chosen': -0.8430452346801758, 'logits/rejected': -0.9655014276504517, 'epoch': 1.89}\n",
            "{'loss': 0.4242, 'grad_norm': 4.376532077789307, 'learning_rate': 0.00013000000000000002, 'rewards/chosen': 0.7073051929473877, 'rewards/rejected': -0.1597169190645218, 'rewards/accuracies': 0.8333333730697632, 'rewards/margins': 0.8670220971107483, 'logps/chosen': -200.559814453125, 'logps/rejected': -126.85111999511719, 'logits/chosen': -0.8949538469314575, 'logits/rejected': -0.8345005512237549, 'epoch': 1.96}\n",
            "{'loss': 0.3801, 'grad_norm': 9.135926246643066, 'learning_rate': 0.000135, 'rewards/chosen': 1.4395607709884644, 'rewards/rejected': -0.38230592012405396, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8218666315078735, 'logps/chosen': -221.33120727539062, 'logps/rejected': -211.52716064453125, 'logits/chosen': -0.70688796043396, 'logits/rejected': -0.7860565185546875, 'epoch': 2.0}\n",
            "{'loss': 0.1135, 'grad_norm': 1.5980517864227295, 'learning_rate': 0.00014000000000000001, 'rewards/chosen': 1.3092292547225952, 'rewards/rejected': -1.1648435592651367, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4740729331970215, 'logps/chosen': -208.73785400390625, 'logps/rejected': -205.80441284179688, 'logits/chosen': -0.6474164128303528, 'logits/rejected': -0.7030031085014343, 'epoch': 2.07}\n",
            "{'loss': 0.1419, 'grad_norm': 2.1466429233551025, 'learning_rate': 0.000145, 'rewards/chosen': 0.8769317865371704, 'rewards/rejected': -1.5840895175933838, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4610214233398438, 'logps/chosen': -207.5492401123047, 'logps/rejected': -277.64166259765625, 'logits/chosen': -0.8795682191848755, 'logits/rejected': -0.9413681626319885, 'epoch': 2.15}\n",
            " 30% 30/100 [07:23<12:12, 10.47s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.72it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.73it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.84it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.39it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:02,  2.48it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:06,  2.32it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.20it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:08,  2.22it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:12,  2.07it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:06,  2.26it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:02,  2.38it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:04,  2.26it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:09,  2.11it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:09,  2.08it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.23it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.44it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.25it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.45it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.62it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.63it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.43it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.69it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.71it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.72it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.25it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:06,  2.01it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:59,  2.23it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.51it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.58it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.30it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.49it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.77it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.76it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.96it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:43,  2.81it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:44,  2.72it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:43,  2.79it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:48,  2.47it/s]\u001b[A\n",
            " 26% 42/160 [00:17<00:46,  2.52it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:51,  2.28it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:55,  2.09it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:51,  2.25it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:54,  2.10it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.08it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.45it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.54it/s]\u001b[A\n",
            " 32% 51/160 [00:21<00:44,  2.46it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.25it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:48,  2.19it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.39it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.84it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.83it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.79it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:39,  2.51it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.34it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.45it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.22it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.38it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:37,  2.52it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:41,  2.26it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:43,  2.14it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:42,  2.12it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:36,  2.50it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:33,  2.66it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.33it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.17it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.29it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.66it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.43it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.22it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.17it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.47it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:32,  2.44it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.39it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.20it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:32,  2.34it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.32it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.23it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.35it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.34it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.54it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.60it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.75it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.96it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.51it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.74it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.78it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:25,  2.56it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:27,  2.35it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:25,  2.49it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:23,  2.65it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.66it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.43it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.39it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.10it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.15it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.05it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.04it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:23,  2.27it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.39it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.20it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.24it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.02it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.95it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.84it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.03it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.97it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.22it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.35it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.33it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.45it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.19it/s]\u001b[A\n",
            " 76% 121/160 [00:51<00:17,  2.17it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:16,  2.35it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.21it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:15,  2.33it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.44it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:14,  2.31it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.41it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.50it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.67it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.36it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.16it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.19it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.48it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.65it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.33it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.32it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.31it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.41it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.60it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.51it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.56it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.38it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.64it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.70it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.57it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.39it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.47it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.37it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.07it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.18it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.06it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.13it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.27it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.39it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.03it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.04it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.04it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.11it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.1311558187007904, 'eval_runtime': 68.0349, 'eval_samples_per_second': 2.352, 'eval_steps_per_second': 2.352, 'eval_rewards/chosen': 1.319617509841919, 'eval_rewards/rejected': -1.2501180171966553, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 2.569735288619995, 'eval_logps/chosen': -199.87826538085938, 'eval_logps/rejected': -197.92251586914062, 'eval_logits/chosen': -0.7622787356376648, 'eval_logits/rejected': -0.7559030652046204, 'epoch': 2.15}\n",
            " 30% 30/100 [08:31<12:12, 10.47s/it]\n",
            "100% 160/160 [01:07<00:00,  2.16it/s]\u001b[A\n",
            "{'loss': 0.1299, 'grad_norm': 1.9197286367416382, 'learning_rate': 0.00015, 'rewards/chosen': 1.0508346557617188, 'rewards/rejected': -1.2761132717132568, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3269481658935547, 'logps/chosen': -161.0421600341797, 'logps/rejected': -211.7556915283203, 'logits/chosen': -0.7769990563392639, 'logits/rejected': -0.5607917904853821, 'epoch': 2.22}\n",
            "{'loss': 0.1203, 'grad_norm': 2.283600330352783, 'learning_rate': 0.000155, 'rewards/chosen': 1.47518789768219, 'rewards/rejected': -1.1455227136611938, 'rewards/accuracies': 1.0, 'rewards/margins': 2.620710849761963, 'logps/chosen': -250.65969848632812, 'logps/rejected': -185.50027465820312, 'logits/chosen': -0.7901750802993774, 'logits/rejected': -0.8932924270629883, 'epoch': 2.3}\n",
            "{'loss': 0.122, 'grad_norm': 2.175112009048462, 'learning_rate': 0.00016, 'rewards/chosen': 1.6637673377990723, 'rewards/rejected': -1.258520483970642, 'rewards/accuracies': 1.0, 'rewards/margins': 2.922287940979004, 'logps/chosen': -209.44769287109375, 'logps/rejected': -151.30392456054688, 'logits/chosen': -0.8576610684394836, 'logits/rejected': -0.6526184678077698, 'epoch': 2.37}\n",
            "{'loss': 0.1568, 'grad_norm': 2.656482219696045, 'learning_rate': 0.000165, 'rewards/chosen': 1.249293565750122, 'rewards/rejected': -1.8959565162658691, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1452503204345703, 'logps/chosen': -205.0285186767578, 'logps/rejected': -223.39065551757812, 'logits/chosen': -0.7509797811508179, 'logits/rejected': -0.8069111108779907, 'epoch': 2.44}\n",
            "{'loss': 0.1043, 'grad_norm': 1.6083922386169434, 'learning_rate': 0.00017, 'rewards/chosen': 1.324170708656311, 'rewards/rejected': -1.5857359170913696, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9099066257476807, 'logps/chosen': -202.06773376464844, 'logps/rejected': -159.92100524902344, 'logits/chosen': -0.5808325409889221, 'logits/rejected': -0.7165881991386414, 'epoch': 2.52}\n",
            "{'loss': 0.1158, 'grad_norm': 2.5318520069122314, 'learning_rate': 0.000175, 'rewards/chosen': 1.2849974632263184, 'rewards/rejected': -1.975585699081421, 'rewards/accuracies': 1.0, 'rewards/margins': 3.26058292388916, 'logps/chosen': -222.4179229736328, 'logps/rejected': -200.30105590820312, 'logits/chosen': -0.809337854385376, 'logits/rejected': -0.7525913715362549, 'epoch': 2.59}\n",
            "{'loss': 0.0859, 'grad_norm': 1.3653903007507324, 'learning_rate': 0.00017999999999999998, 'rewards/chosen': 1.065234661102295, 'rewards/rejected': -2.548541784286499, 'rewards/accuracies': 1.0, 'rewards/margins': 3.613776683807373, 'logps/chosen': -146.3040771484375, 'logps/rejected': -185.79367065429688, 'logits/chosen': -0.5887418985366821, 'logits/rejected': -0.7442827820777893, 'epoch': 2.67}\n",
            "{'loss': 0.071, 'grad_norm': 1.3551408052444458, 'learning_rate': 0.000185, 'rewards/chosen': 0.8903346061706543, 'rewards/rejected': -2.902221202850342, 'rewards/accuracies': 1.0, 'rewards/margins': 3.792555570602417, 'logps/chosen': -147.2208251953125, 'logps/rejected': -253.84922790527344, 'logits/chosen': -0.9112492799758911, 'logits/rejected': -0.7179369330406189, 'epoch': 2.74}\n",
            "{'loss': 0.0879, 'grad_norm': 1.791832685470581, 'learning_rate': 0.00019, 'rewards/chosen': 1.1450691223144531, 'rewards/rejected': -2.3272833824157715, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4723525047302246, 'logps/chosen': -217.10035705566406, 'logps/rejected': -189.50180053710938, 'logits/chosen': -0.838889479637146, 'logits/rejected': -0.9557673931121826, 'epoch': 2.81}\n",
            "{'loss': 0.0661, 'grad_norm': 1.5666654109954834, 'learning_rate': 0.00019500000000000002, 'rewards/chosen': 1.0864641666412354, 'rewards/rejected': -2.408416271209717, 'rewards/accuracies': 1.0, 'rewards/margins': 3.494880199432373, 'logps/chosen': -154.7310333251953, 'logps/rejected': -141.18609619140625, 'logits/chosen': -0.8616190552711487, 'logits/rejected': -0.9531154036521912, 'epoch': 2.89}\n",
            " 40% 40/100 [10:16<10:55, 10.92s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.76it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.76it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.89it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.39it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.33it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.25it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.27it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.13it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.35it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.44it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:04,  2.29it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.13it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.10it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.24it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:59,  2.41it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:04,  2.20it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:59,  2.37it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:55,  2.51it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:54,  2.57it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:57,  2.40it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:51,  2.67it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.69it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.71it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.24it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:06,  2.01it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.24it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.49it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.57it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.29it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.48it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.65it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.78it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.77it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.97it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.90it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.90it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.66it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.59it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.31it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.14it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.28it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:54,  2.09it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:55,  2.04it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:48,  2.30it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:47,  2.35it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:44,  2.45it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:45,  2.40it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.21it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:49,  2.17it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.38it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.35it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.44it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.51it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.85it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.83it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.79it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.52it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.35it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.44it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.42it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.17it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.70it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.36it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.17it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.27it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:33,  2.57it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:35,  2.34it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:39,  2.12it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:39,  2.08it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:33,  2.38it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:34,  2.35it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.35it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.17it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.32it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.31it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.22it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.36it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.35it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.55it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.60it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.76it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.96it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.51it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.75it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.86it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.65it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.43it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.62it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.75it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.48it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.41it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.12it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.01it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:27,  1.98it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:24,  2.17it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.32it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:24,  2.17it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:23,  2.21it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:24,  2.01it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.95it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:25,  1.85it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.03it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.98it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.23it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.36it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.34it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.45it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.22it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.43it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.29it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.49it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.34it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.51it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.68it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.32it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.28it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.08it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.13it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.42it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.62it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.31it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.30it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.30it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.40it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.56it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.48it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.54it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.36it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.63it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.68it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.54it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.37it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.45it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.10it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.25it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.10it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.16it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.30it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.41it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.03it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.00it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:01,  1.97it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.03it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.02193363569676876, 'eval_runtime': 68.0734, 'eval_samples_per_second': 2.35, 'eval_steps_per_second': 2.35, 'eval_rewards/chosen': 1.6002572774887085, 'eval_rewards/rejected': -3.6612327098846436, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 5.261490821838379, 'eval_logps/chosen': -197.0718994140625, 'eval_logps/rejected': -222.03366088867188, 'eval_logits/chosen': -0.9392549395561218, 'eval_logits/rejected': -0.9120491743087769, 'epoch': 2.89}\n",
            " 40% 40/100 [11:24<10:55, 10.92s/it]\n",
            "100% 160/160 [01:07<00:00,  2.10it/s]\u001b[A\n",
            "{'loss': 0.0207, 'grad_norm': 0.6887677907943726, 'learning_rate': 0.0002, 'rewards/chosen': 1.4564837217330933, 'rewards/rejected': -3.582531690597534, 'rewards/accuracies': 1.0, 'rewards/margins': 5.039015769958496, 'logps/chosen': -279.7249755859375, 'logps/rejected': -300.3459167480469, 'logits/chosen': -1.0147426128387451, 'logits/rejected': -0.9012007117271423, 'epoch': 2.96}\n",
            "{'loss': 0.0368, 'grad_norm': 2.052335023880005, 'learning_rate': 0.000205, 'rewards/chosen': 1.167510986328125, 'rewards/rejected': -2.1315040588378906, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2990150451660156, 'logps/chosen': -199.2104949951172, 'logps/rejected': -127.56443786621094, 'logits/chosen': -1.440274715423584, 'logits/rejected': -1.833235502243042, 'epoch': 3.0}\n",
            "{'loss': 0.015, 'grad_norm': 0.43253207206726074, 'learning_rate': 0.00021, 'rewards/chosen': 1.2464932203292847, 'rewards/rejected': -4.159891128540039, 'rewards/accuracies': 1.0, 'rewards/margins': 5.406384468078613, 'logps/chosen': -154.72250366210938, 'logps/rejected': -184.36328125, 'logits/chosen': -0.8430665731430054, 'logits/rejected': -0.971118152141571, 'epoch': 3.07}\n",
            "{'loss': 0.0071, 'grad_norm': 0.2117452621459961, 'learning_rate': 0.000215, 'rewards/chosen': 1.3858635425567627, 'rewards/rejected': -4.432833671569824, 'rewards/accuracies': 1.0, 'rewards/margins': 5.818696975708008, 'logps/chosen': -169.12088012695312, 'logps/rejected': -164.57388305664062, 'logits/chosen': -1.263408899307251, 'logits/rejected': -1.2431007623672485, 'epoch': 3.15}\n",
            "{'loss': 0.0201, 'grad_norm': 0.5023024678230286, 'learning_rate': 0.00022, 'rewards/chosen': 1.3271479606628418, 'rewards/rejected': -4.621387958526611, 'rewards/accuracies': 1.0, 'rewards/margins': 5.948536396026611, 'logps/chosen': -127.92080688476562, 'logps/rejected': -177.97274780273438, 'logits/chosen': -1.3349424600601196, 'logits/rejected': -0.9902634620666504, 'epoch': 3.22}\n",
            "{'loss': 0.0081, 'grad_norm': 0.3938407003879547, 'learning_rate': 0.00022500000000000002, 'rewards/chosen': 0.548003613948822, 'rewards/rejected': -7.040546417236328, 'rewards/accuracies': 1.0, 'rewards/margins': 7.58854866027832, 'logps/chosen': -193.533935546875, 'logps/rejected': -283.8634033203125, 'logits/chosen': -1.1715728044509888, 'logits/rejected': -1.1714264154434204, 'epoch': 3.3}\n",
            "{'loss': 0.0026, 'grad_norm': 0.18321344256401062, 'learning_rate': 0.00023, 'rewards/chosen': 1.3987590074539185, 'rewards/rejected': -6.127902984619141, 'rewards/accuracies': 1.0, 'rewards/margins': 7.526662349700928, 'logps/chosen': -188.56854248046875, 'logps/rejected': -224.802490234375, 'logits/chosen': -1.3145334720611572, 'logits/rejected': -1.4786497354507446, 'epoch': 3.37}\n",
            "{'loss': 0.1496, 'grad_norm': 14.73549747467041, 'learning_rate': 0.000235, 'rewards/chosen': -0.4112936854362488, 'rewards/rejected': -9.297385215759277, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 8.886091232299805, 'logps/chosen': -223.82505798339844, 'logps/rejected': -405.42523193359375, 'logits/chosen': -1.457334280014038, 'logits/rejected': -1.2991411685943604, 'epoch': 3.44}\n",
            "{'loss': 0.0403, 'grad_norm': 1.7522746324539185, 'learning_rate': 0.00024, 'rewards/chosen': 1.319753885269165, 'rewards/rejected': -4.933034896850586, 'rewards/accuracies': 1.0, 'rewards/margins': 6.252788543701172, 'logps/chosen': -179.20675659179688, 'logps/rejected': -159.5813446044922, 'logits/chosen': -1.3141579627990723, 'logits/rejected': -1.438680648803711, 'epoch': 3.52}\n",
            "{'loss': 0.0007, 'grad_norm': 0.05760929360985756, 'learning_rate': 0.000245, 'rewards/chosen': 1.7426223754882812, 'rewards/rejected': -7.804298400878906, 'rewards/accuracies': 1.0, 'rewards/margins': 9.546920776367188, 'logps/chosen': -252.97344970703125, 'logps/rejected': -295.0545654296875, 'logits/chosen': -1.4166381359100342, 'logits/rejected': -1.5361013412475586, 'epoch': 3.59}\n",
            " 50% 50/100 [12:59<09:14, 11.09s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.74it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.74it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.87it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.39it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.50it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.33it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.23it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.26it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.12it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.34it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.45it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.31it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:09,  2.10it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:03,  2.25it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.45it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.24it/s]\u001b[A\n",
            " 12% 19/160 [00:07<00:57,  2.45it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.60it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.64it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:57,  2.39it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:52,  2.60it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:52,  2.60it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:52,  2.59it/s]\u001b[A\n",
            " 16% 26/160 [00:10<01:01,  2.16it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:07,  1.96it/s]\u001b[A\n",
            " 18% 28/160 [00:11<01:00,  2.20it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.48it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:51,  2.54it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.28it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.47it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:48,  2.64it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:45,  2.76it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.75it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:42,  2.90it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.91it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.68it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.59it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.30it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.13it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.27it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:54,  2.11it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.08it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.37it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.45it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.54it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:45,  2.40it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:49,  2.18it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:50,  2.12it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:46,  2.29it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:45,  2.30it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:43,  2.39it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.47it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.81it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.79it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.78it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:39,  2.51it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.34it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.44it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.22it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.42it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.61it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:42,  2.16it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.54it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.70it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.35it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.19it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.31it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.67it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.44it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.22it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.18it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:33,  2.43it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.36it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.33it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:36,  2.13it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.27it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:33,  2.28it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:34,  2.19it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.33it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.33it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.53it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.59it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.75it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.95it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.50it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.71it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.83it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.63it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.43it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.61it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.48it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.41it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.17it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.07it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.04it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.27it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.40it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.19it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:23,  2.21it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:25,  1.97it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.92it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.83it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.02it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.96it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.21it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.35it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.33it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.44it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.22it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.24it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.45it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.30it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:15,  2.40it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.49it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.33it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.50it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.67it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.37it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.35it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.16it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.20it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.47it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.59it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.26it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:10,  2.24it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.22it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.34it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.53it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.47it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.53it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.36it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.62it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.67it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.55it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.48it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.43it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.26it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.11it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.17it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.31it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.42it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.04it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.04it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.03it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.10it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.003840762423351407, 'eval_runtime': 67.9358, 'eval_samples_per_second': 2.355, 'eval_steps_per_second': 2.355, 'eval_rewards/chosen': 0.9240753054618835, 'eval_rewards/rejected': -7.934426784515381, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 8.858502388000488, 'eval_logps/chosen': -203.83370971679688, 'eval_logps/rejected': -264.7655944824219, 'eval_logits/chosen': -1.4178928136825562, 'eval_logits/rejected': -1.3924270868301392, 'epoch': 3.59}\n",
            " 50% 50/100 [14:07<09:14, 11.09s/it]\n",
            "100% 160/160 [01:07<00:00,  2.18it/s]\u001b[A\n",
            "{'loss': 0.0017, 'grad_norm': 0.08718899637460709, 'learning_rate': 0.00025, 'rewards/chosen': 0.7762786149978638, 'rewards/rejected': -7.993272304534912, 'rewards/accuracies': 1.0, 'rewards/margins': 8.769552230834961, 'logps/chosen': -202.83163452148438, 'logps/rejected': -300.2876281738281, 'logits/chosen': -1.1902512311935425, 'logits/rejected': -1.3143032789230347, 'epoch': 3.67}\n",
            "{'loss': 0.0148, 'grad_norm': 2.67071795463562, 'learning_rate': 0.000255, 'rewards/chosen': 1.6439005136489868, 'rewards/rejected': -7.186528205871582, 'rewards/accuracies': 1.0, 'rewards/margins': 8.830428123474121, 'logps/chosen': -211.62403869628906, 'logps/rejected': -188.22377014160156, 'logits/chosen': -1.678001880645752, 'logits/rejected': -1.8135201930999756, 'epoch': 3.74}\n",
            "{'loss': 0.0076, 'grad_norm': 0.5577853322029114, 'learning_rate': 0.00026000000000000003, 'rewards/chosen': 0.6559415459632874, 'rewards/rejected': -8.026606559753418, 'rewards/accuracies': 1.0, 'rewards/margins': 8.682547569274902, 'logps/chosen': -222.1813201904297, 'logps/rejected': -279.7309265136719, 'logits/chosen': -1.3364925384521484, 'logits/rejected': -1.264035940170288, 'epoch': 3.81}\n",
            "{'loss': 0.0105, 'grad_norm': 1.779856562614441, 'learning_rate': 0.00026500000000000004, 'rewards/chosen': 0.7364276647567749, 'rewards/rejected': -8.535848617553711, 'rewards/accuracies': 1.0, 'rewards/margins': 9.272276878356934, 'logps/chosen': -237.77011108398438, 'logps/rejected': -293.1186218261719, 'logits/chosen': -1.4014209508895874, 'logits/rejected': -1.0743623971939087, 'epoch': 3.89}\n",
            "{'loss': 0.001, 'grad_norm': 0.08455494791269302, 'learning_rate': 0.00027, 'rewards/chosen': 0.46257466077804565, 'rewards/rejected': -8.069189071655273, 'rewards/accuracies': 1.0, 'rewards/margins': 8.53176498413086, 'logps/chosen': -272.5798645019531, 'logps/rejected': -304.7550048828125, 'logits/chosen': -1.1637649536132812, 'logits/rejected': -0.9872615933418274, 'epoch': 3.96}\n",
            "{'loss': 0.0, 'grad_norm': 0.001443770481273532, 'learning_rate': 0.000275, 'rewards/chosen': -1.6503219604492188, 'rewards/rejected': -13.442909240722656, 'rewards/accuracies': 1.0, 'rewards/margins': 11.792586326599121, 'logps/chosen': -238.24801635742188, 'logps/rejected': -365.7951965332031, 'logits/chosen': -1.8726989030838013, 'logits/rejected': -1.2885775566101074, 'epoch': 4.0}\n",
            "{'loss': 0.0004, 'grad_norm': 0.027760099619627, 'learning_rate': 0.00028000000000000003, 'rewards/chosen': 0.1826622486114502, 'rewards/rejected': -9.368179321289062, 'rewards/accuracies': 1.0, 'rewards/margins': 9.55084228515625, 'logps/chosen': -247.79449462890625, 'logps/rejected': -323.34320068359375, 'logits/chosen': -1.4667065143585205, 'logits/rejected': -1.6296088695526123, 'epoch': 4.07}\n",
            "{'loss': 0.0005, 'grad_norm': 0.025384651497006416, 'learning_rate': 0.000285, 'rewards/chosen': 2.6630072593688965, 'rewards/rejected': -8.113811492919922, 'rewards/accuracies': 1.0, 'rewards/margins': 10.776819229125977, 'logps/chosen': -257.6492614746094, 'logps/rejected': -267.4551696777344, 'logits/chosen': -1.4453212022781372, 'logits/rejected': -1.687730073928833, 'epoch': 4.15}\n",
            "{'loss': 0.0026, 'grad_norm': 0.12435755878686905, 'learning_rate': 0.00029, 'rewards/chosen': 2.7592787742614746, 'rewards/rejected': -5.774568557739258, 'rewards/accuracies': 1.0, 'rewards/margins': 8.53384780883789, 'logps/chosen': -224.21853637695312, 'logps/rejected': -189.9855499267578, 'logits/chosen': -1.5583739280700684, 'logits/rejected': -1.5074520111083984, 'epoch': 4.22}\n",
            "{'loss': 0.0022, 'grad_norm': 0.09958340972661972, 'learning_rate': 0.000295, 'rewards/chosen': 1.0692873001098633, 'rewards/rejected': -7.399586200714111, 'rewards/accuracies': 1.0, 'rewards/margins': 8.468873977661133, 'logps/chosen': -145.72622680664062, 'logps/rejected': -226.60293579101562, 'logits/chosen': -1.61238431930542, 'logits/rejected': -1.531465768814087, 'epoch': 4.3}\n",
            " 60% 60/100 [15:47<06:59, 10.49s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.78it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.75it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.87it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.40it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.49it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.33it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.27it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.13it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.35it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.46it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.31it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.14it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.11it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.24it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.45it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:04,  2.21it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:59,  2.39it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:55,  2.54it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:54,  2.55it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:57,  2.39it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:51,  2.66it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.69it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.71it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.24it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:06,  2.01it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:59,  2.22it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.50it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.58it/s]\u001b[A\n",
            " 19% 31/160 [00:12<00:55,  2.30it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.49it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.78it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.75it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:14<00:42,  2.90it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.91it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.68it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.57it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.60it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.32it/s]\u001b[A\n",
            " 28% 44/160 [00:17<00:54,  2.14it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.28it/s]\u001b[A\n",
            " 29% 46/160 [00:18<00:53,  2.12it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:55,  2.04it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:48,  2.30it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:47,  2.36it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:45,  2.43it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:46,  2.35it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:49,  2.17it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:50,  2.14it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:45,  2.35it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:45,  2.32it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:43,  2.41it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.49it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.83it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.82it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.80it/s]\u001b[A\n",
            " 38% 61/160 [00:24<00:39,  2.52it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.36it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.44it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.42it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.23it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.18it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.70it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.36it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.18it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.29it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:32,  2.64it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:35,  2.37it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:38,  2.17it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:38,  2.11it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:34,  2.36it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.36it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.35it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.17it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.32it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.31it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.22it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.35it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.35it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.54it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.60it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.75it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.94it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.49it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.73it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.85it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.65it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.44it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.62it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.48it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.42it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.16it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.03it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:27,  1.99it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:24,  2.20it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:23,  2.28it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:24,  2.14it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:23,  2.20it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:25,  2.00it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.94it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.85it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.03it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.97it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.22it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.36it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.33it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.23it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.44it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.22it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.24it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.44it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.30it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.50it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.34it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.51it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.68it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.31it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.11it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.11it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.36it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.56it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.29it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:10,  2.29it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.29it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.38it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.57it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.48it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.53it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.37it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.62it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.67it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.56it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.47it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.42it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.25it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.09it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.14it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.28it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.40it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.03it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.03it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:01,  1.99it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.05it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.0008185174083337188, 'eval_runtime': 68.0412, 'eval_samples_per_second': 2.352, 'eval_steps_per_second': 2.352, 'eval_rewards/chosen': 1.6977558135986328, 'eval_rewards/rejected': -7.72269344329834, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 9.420449256896973, 'eval_logps/chosen': -196.09689331054688, 'eval_logps/rejected': -262.6482849121094, 'eval_logits/chosen': -1.454111099243164, 'eval_logits/rejected': -1.4268916845321655, 'epoch': 4.3}\n",
            " 60% 60/100 [16:55<06:59, 10.49s/it]\n",
            "100% 160/160 [01:07<00:00,  2.10it/s]\u001b[A\n",
            "{'loss': 0.0002, 'grad_norm': 0.0118625583127141, 'learning_rate': 0.0003, 'rewards/chosen': 0.66826331615448, 'rewards/rejected': -9.271980285644531, 'rewards/accuracies': 1.0, 'rewards/margins': 9.9402437210083, 'logps/chosen': -228.00624084472656, 'logps/rejected': -346.6433410644531, 'logits/chosen': -1.5935667753219604, 'logits/rejected': -1.431252360343933, 'epoch': 4.37}\n",
            "{'loss': 0.001, 'grad_norm': 0.06433215737342834, 'learning_rate': 0.000305, 'rewards/chosen': 2.6172051429748535, 'rewards/rejected': -8.197548866271973, 'rewards/accuracies': 1.0, 'rewards/margins': 10.814753532409668, 'logps/chosen': -165.8990936279297, 'logps/rejected': -248.28182983398438, 'logits/chosen': -1.6222972869873047, 'logits/rejected': -1.355177879333496, 'epoch': 4.44}\n",
            "{'loss': 0.0011, 'grad_norm': 0.06626461446285248, 'learning_rate': 0.00031, 'rewards/chosen': 1.6378726959228516, 'rewards/rejected': -7.4815754890441895, 'rewards/accuracies': 1.0, 'rewards/margins': 9.119447708129883, 'logps/chosen': -226.05307006835938, 'logps/rejected': -266.4580383300781, 'logits/chosen': -1.1927978992462158, 'logits/rejected': -1.2110214233398438, 'epoch': 4.52}\n",
            "{'loss': 0.0019, 'grad_norm': 0.05273347347974777, 'learning_rate': 0.000315, 'rewards/chosen': 1.3606348037719727, 'rewards/rejected': -8.227975845336914, 'rewards/accuracies': 1.0, 'rewards/margins': 9.58860969543457, 'logps/chosen': -148.00791931152344, 'logps/rejected': -299.0328369140625, 'logits/chosen': -1.6490590572357178, 'logits/rejected': -1.5678176879882812, 'epoch': 4.59}\n",
            "{'loss': 0.0004, 'grad_norm': 0.015282065607607365, 'learning_rate': 0.00032, 'rewards/chosen': 1.9809294939041138, 'rewards/rejected': -7.695308208465576, 'rewards/accuracies': 1.0, 'rewards/margins': 9.676237106323242, 'logps/chosen': -136.8345947265625, 'logps/rejected': -227.73422241210938, 'logits/chosen': -1.5944775342941284, 'logits/rejected': -1.4970864057540894, 'epoch': 4.67}\n",
            "{'loss': 0.0004, 'grad_norm': 0.02454816736280918, 'learning_rate': 0.00032500000000000004, 'rewards/chosen': 0.8440622091293335, 'rewards/rejected': -8.235090255737305, 'rewards/accuracies': 1.0, 'rewards/margins': 9.07915210723877, 'logps/chosen': -251.00172424316406, 'logps/rejected': -320.55328369140625, 'logits/chosen': -1.2227139472961426, 'logits/rejected': -1.1832870244979858, 'epoch': 4.74}\n",
            "{'loss': 0.0004, 'grad_norm': 0.01868627592921257, 'learning_rate': 0.00033, 'rewards/chosen': 2.770691156387329, 'rewards/rejected': -6.906771183013916, 'rewards/accuracies': 1.0, 'rewards/margins': 9.677461624145508, 'logps/chosen': -217.92404174804688, 'logps/rejected': -218.81399536132812, 'logits/chosen': -1.4122806787490845, 'logits/rejected': -1.4542609453201294, 'epoch': 4.81}\n",
            "{'loss': 0.0007, 'grad_norm': 0.039438407868146896, 'learning_rate': 0.000335, 'rewards/chosen': 1.826357126235962, 'rewards/rejected': -7.302040100097656, 'rewards/accuracies': 1.0, 'rewards/margins': 9.128396987915039, 'logps/chosen': -124.61663818359375, 'logps/rejected': -212.9600372314453, 'logits/chosen': -1.3965346813201904, 'logits/rejected': -1.1818885803222656, 'epoch': 4.89}\n",
            "{'loss': 0.001, 'grad_norm': 0.045157574117183685, 'learning_rate': 0.00034, 'rewards/chosen': 1.5081689357757568, 'rewards/rejected': -7.275449275970459, 'rewards/accuracies': 1.0, 'rewards/margins': 8.783617973327637, 'logps/chosen': -187.0462646484375, 'logps/rejected': -274.3853759765625, 'logits/chosen': -1.1871867179870605, 'logits/rejected': -1.2073378562927246, 'epoch': 4.96}\n",
            "{'loss': 0.0002, 'grad_norm': 0.01724966987967491, 'learning_rate': 0.000345, 'rewards/chosen': -2.578627109527588, 'rewards/rejected': -14.429244995117188, 'rewards/accuracies': 1.0, 'rewards/margins': 11.850617408752441, 'logps/chosen': -315.02191162109375, 'logps/rejected': -446.59564208984375, 'logits/chosen': -1.6948933601379395, 'logits/rejected': -1.5969793796539307, 'epoch': 5.0}\n",
            " 70% 70/100 [18:34<04:32,  9.09s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.76it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:56,  2.76it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.86it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:04,  2.40it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:01,  2.48it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.33it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.25it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:11,  2.11it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.33it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.44it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:04,  2.29it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.13it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:09,  2.08it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:05,  2.20it/s]\u001b[A\n",
            " 11% 17/160 [00:07<01:00,  2.37it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:05,  2.17it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:59,  2.38it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:54,  2.56it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:53,  2.61it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:57,  2.41it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:51,  2.68it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.70it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.72it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.24it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:06,  2.01it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:58,  2.24it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.50it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.57it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.30it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.48it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:47,  2.66it/s]\u001b[A\n",
            " 21% 34/160 [00:13<00:45,  2.77it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:46,  2.70it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.92it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:42,  2.86it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.78it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:42,  2.87it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.66it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:47,  2.53it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:51,  2.29it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:55,  2.08it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:52,  2.19it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:56,  2.03it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:56,  1.99it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:49,  2.28it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:46,  2.38it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:44,  2.48it/s]\u001b[A\n",
            " 32% 51/160 [00:21<00:45,  2.42it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.21it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:49,  2.16it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.37it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.35it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:42,  2.43it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:41,  2.50it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:35,  2.84it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:35,  2.82it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:35,  2.78it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:39,  2.51it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.34it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.43it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.41it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.59it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:40,  2.31it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.20it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:42,  2.16it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.52it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:33,  2.68it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.32it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.12it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:38,  2.23it/s]\u001b[A\n",
            " 47% 75/160 [00:31<00:34,  2.48it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:37,  2.26it/s]\u001b[A\n",
            " 48% 77/160 [00:32<00:39,  2.12it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:38,  2.11it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:33,  2.40it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.38it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.36it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.17it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.32it/s]\u001b[A\n",
            " 52% 84/160 [00:35<00:32,  2.31it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:33,  2.21it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.33it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.32it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.52it/s]\u001b[A\n",
            " 56% 89/160 [00:37<00:27,  2.59it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.74it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.95it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.49it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.74it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.85it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.61it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.41it/s]\u001b[A\n",
            " 61% 97/160 [00:40<00:24,  2.59it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.74it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.74it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.47it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.36it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:28,  2.06it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:27,  2.10it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:28,  1.97it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:27,  1.99it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:24,  2.23it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.36it/s]\u001b[A\n",
            " 68% 108/160 [00:45<00:23,  2.19it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.23it/s]\u001b[A\n",
            " 69% 110/160 [00:46<00:24,  2.02it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.95it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.85it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.03it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.97it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.20it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.34it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.33it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.20it/s]\u001b[A\n",
            " 74% 119/160 [00:50<00:16,  2.42it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.21it/s]\u001b[A\n",
            " 76% 121/160 [00:51<00:17,  2.22it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.42it/s]\u001b[A\n",
            " 77% 123/160 [00:52<00:16,  2.29it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:15,  2.38it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.47it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:14,  2.30it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:14,  2.35it/s]\u001b[A\n",
            " 80% 128/160 [00:54<00:13,  2.40it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:12,  2.55it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:13,  2.25it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.25it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.09it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.15it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.43it/s]\u001b[A\n",
            " 84% 135/160 [00:57<00:09,  2.62it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.31it/s]\u001b[A\n",
            " 86% 137/160 [00:58<00:09,  2.30it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.30it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.40it/s]\u001b[A\n",
            " 88% 140/160 [00:59<00:07,  2.57it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.48it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.53it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.36it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.62it/s]\u001b[A\n",
            " 91% 145/160 [01:01<00:05,  2.66it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.55it/s]\u001b[A\n",
            " 92% 147/160 [01:02<00:05,  2.36it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.45it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.10it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.24it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.09it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:05<00:02,  2.29it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.35it/s]\u001b[A\n",
            " 98% 156/160 [01:06<00:02,  2.00it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  1.96it/s]\u001b[A\n",
            " 99% 158/160 [01:07<00:01,  1.95it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.05it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.00041065519326366484, 'eval_runtime': 68.4336, 'eval_samples_per_second': 2.338, 'eval_steps_per_second': 2.338, 'eval_rewards/chosen': 1.5406075716018677, 'eval_rewards/rejected': -8.301312446594238, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 9.841920852661133, 'eval_logps/chosen': -197.6683807373047, 'eval_logps/rejected': -268.4344787597656, 'eval_logits/chosen': -1.5820913314819336, 'eval_logits/rejected': -1.5534579753875732, 'epoch': 5.0}\n",
            " 70% 70/100 [19:42<04:32,  9.09s/it]\n",
            "100% 160/160 [01:08<00:00,  2.13it/s]\u001b[A\n",
            "{'loss': 0.0004, 'grad_norm': 0.021763479337096214, 'learning_rate': 0.00035, 'rewards/chosen': 2.907010555267334, 'rewards/rejected': -9.743419647216797, 'rewards/accuracies': 1.0, 'rewards/margins': 12.650430679321289, 'logps/chosen': -261.4501953125, 'logps/rejected': -377.4752502441406, 'logits/chosen': -1.2914199829101562, 'logits/rejected': -1.2091968059539795, 'epoch': 5.07}\n",
            "{'loss': 0.0011, 'grad_norm': 0.03135085850954056, 'learning_rate': 0.000355, 'rewards/chosen': 1.6256179809570312, 'rewards/rejected': -7.537795066833496, 'rewards/accuracies': 1.0, 'rewards/margins': 9.163414001464844, 'logps/chosen': -192.07676696777344, 'logps/rejected': -232.22279357910156, 'logits/chosen': -1.5666909217834473, 'logits/rejected': -1.6250064373016357, 'epoch': 5.15}\n",
            "{'loss': 0.0004, 'grad_norm': 0.019915111362934113, 'learning_rate': 0.00035999999999999997, 'rewards/chosen': 0.7669994235038757, 'rewards/rejected': -8.604023933410645, 'rewards/accuracies': 1.0, 'rewards/margins': 9.371023178100586, 'logps/chosen': -183.51768493652344, 'logps/rejected': -258.8189697265625, 'logits/chosen': -1.6237387657165527, 'logits/rejected': -1.5266704559326172, 'epoch': 5.22}\n",
            "{'loss': 0.0001, 'grad_norm': 0.009218430146574974, 'learning_rate': 0.000365, 'rewards/chosen': 0.9577958583831787, 'rewards/rejected': -8.721065521240234, 'rewards/accuracies': 1.0, 'rewards/margins': 9.678861618041992, 'logps/chosen': -252.28140258789062, 'logps/rejected': -317.9741516113281, 'logits/chosen': -1.7292858362197876, 'logits/rejected': -1.5053280591964722, 'epoch': 5.3}\n",
            "{'loss': 0.0002, 'grad_norm': 0.007176154758781195, 'learning_rate': 0.00037, 'rewards/chosen': 1.4725075960159302, 'rewards/rejected': -7.8334550857543945, 'rewards/accuracies': 1.0, 'rewards/margins': 9.305963516235352, 'logps/chosen': -196.0438232421875, 'logps/rejected': -258.842529296875, 'logits/chosen': -1.683019757270813, 'logits/rejected': -1.6547497510910034, 'epoch': 5.37}\n",
            "{'loss': 0.0003, 'grad_norm': 0.01863371767103672, 'learning_rate': 0.000375, 'rewards/chosen': 1.4982414245605469, 'rewards/rejected': -7.8540239334106445, 'rewards/accuracies': 1.0, 'rewards/margins': 9.352265357971191, 'logps/chosen': -206.20008850097656, 'logps/rejected': -234.20037841796875, 'logits/chosen': -1.4313840866088867, 'logits/rejected': -1.5731714963912964, 'epoch': 5.44}\n",
            "{'loss': 0.0002, 'grad_norm': 0.00918863620609045, 'learning_rate': 0.00038, 'rewards/chosen': 3.0937182903289795, 'rewards/rejected': -7.897751808166504, 'rewards/accuracies': 1.0, 'rewards/margins': 10.991469383239746, 'logps/chosen': -151.07135009765625, 'logps/rejected': -228.5732421875, 'logits/chosen': -1.5821360349655151, 'logits/rejected': -1.5900952816009521, 'epoch': 5.52}\n",
            "{'loss': 0.0001, 'grad_norm': 0.006739544682204723, 'learning_rate': 0.00038500000000000003, 'rewards/chosen': 0.2838943600654602, 'rewards/rejected': -10.152999877929688, 'rewards/accuracies': 1.0, 'rewards/margins': 10.436894416809082, 'logps/chosen': -221.9107666015625, 'logps/rejected': -327.682861328125, 'logits/chosen': -1.687647819519043, 'logits/rejected': -1.7212600708007812, 'epoch': 5.59}\n",
            "{'loss': 0.0002, 'grad_norm': 0.011131854727864265, 'learning_rate': 0.00039000000000000005, 'rewards/chosen': 2.029306173324585, 'rewards/rejected': -7.658524513244629, 'rewards/accuracies': 1.0, 'rewards/margins': 9.687830924987793, 'logps/chosen': -187.43045043945312, 'logps/rejected': -243.76686096191406, 'logits/chosen': -1.578175663948059, 'logits/rejected': -1.8397681713104248, 'epoch': 5.67}\n",
            "{'loss': 0.0001, 'grad_norm': 0.003246147185564041, 'learning_rate': 0.000395, 'rewards/chosen': -0.133560910820961, 'rewards/rejected': -11.439821243286133, 'rewards/accuracies': 1.0, 'rewards/margins': 11.30626106262207, 'logps/chosen': -194.22219848632812, 'logps/rejected': -325.5644226074219, 'logits/chosen': -1.8748723268508911, 'logits/rejected': -1.648056983947754, 'epoch': 5.74}\n",
            " 80% 80/100 [21:28<03:41, 11.09s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:41,  3.81it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.72it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.84it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:05,  2.38it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:02,  2.47it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:05,  2.32it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.23it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.26it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:10,  2.12it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.34it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.44it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.30it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.13it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:09,  2.10it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.24it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.45it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.24it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.44it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.62it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.63it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.44it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.70it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:51,  2.66it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:51,  2.62it/s]\u001b[A\n",
            " 16% 26/160 [00:10<01:01,  2.17it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:08,  1.94it/s]\u001b[A\n",
            " 18% 28/160 [00:11<01:00,  2.18it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:53,  2.46it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:51,  2.54it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.28it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:52,  2.44it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:48,  2.62it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:45,  2.74it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.72it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.95it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:42,  2.89it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.80it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.89it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.67it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 42/160 [00:17<00:45,  2.58it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.30it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.13it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.28it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:53,  2.11it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.07it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.44it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.53it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.44it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.21it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:50,  2.13it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:45,  2.32it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:46,  2.28it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:44,  2.33it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:42,  2.42it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.77it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.77it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.76it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:39,  2.49it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:41,  2.33it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.43it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.22it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.42it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:40,  2.32it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.18it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.53it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:33,  2.68it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.34it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:40,  2.17it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.30it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:32,  2.65it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.44it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.21it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.17it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.46it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:32,  2.43it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.35it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:36,  2.14it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:34,  2.26it/s]\u001b[A\n",
            " 52% 84/160 [00:35<00:34,  2.23it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:34,  2.16it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:32,  2.31it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.31it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.51it/s]\u001b[A\n",
            " 56% 89/160 [00:37<00:27,  2.57it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.72it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.94it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.50it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.75it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.85it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.64it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.43it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.62it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.75it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.48it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.42it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:32,  1.78it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:31,  1.78it/s]\u001b[A\n",
            " 66% 105/160 [00:44<00:29,  1.86it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:25,  2.11it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:23,  2.27it/s]\u001b[A\n",
            " 68% 108/160 [00:45<00:24,  2.13it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:23,  2.16it/s]\u001b[A\n",
            " 69% 110/160 [00:46<00:25,  1.95it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:26,  1.88it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.80it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.00it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:23,  1.94it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.20it/s]\u001b[A\n",
            " 72% 116/160 [00:49<00:18,  2.34it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.32it/s]\u001b[A\n",
            " 74% 118/160 [00:50<00:18,  2.23it/s]\u001b[A\n",
            " 74% 119/160 [00:50<00:16,  2.45it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 121/160 [00:51<00:17,  2.25it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.45it/s]\u001b[A\n",
            " 77% 123/160 [00:52<00:16,  2.31it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:13,  2.50it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:14,  2.35it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.44it/s]\u001b[A\n",
            " 80% 128/160 [00:54<00:12,  2.53it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.69it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.35it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.34it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.14it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.17it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.46it/s]\u001b[A\n",
            " 84% 135/160 [00:57<00:09,  2.63it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.25it/s]\u001b[A\n",
            " 86% 137/160 [00:58<00:10,  2.22it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.21it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:09,  2.31it/s]\u001b[A\n",
            " 88% 140/160 [00:59<00:07,  2.51it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.46it/s]\u001b[A\n",
            " 89% 142/160 [01:00<00:07,  2.52it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.36it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.62it/s]\u001b[A\n",
            " 91% 145/160 [01:01<00:05,  2.67it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.56it/s]\u001b[A\n",
            " 92% 147/160 [01:02<00:05,  2.38it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.48it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.43it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.11it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.26it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.11it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:05<00:02,  2.29it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.39it/s]\u001b[A\n",
            " 98% 156/160 [01:06<00:01,  2.03it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.03it/s]\u001b[A\n",
            " 99% 158/160 [01:07<00:00,  2.03it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.10it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.00020943183335475624, 'eval_runtime': 68.3768, 'eval_samples_per_second': 2.34, 'eval_steps_per_second': 2.34, 'eval_rewards/chosen': 1.4145829677581787, 'eval_rewards/rejected': -9.025115966796875, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 10.439699172973633, 'eval_logps/chosen': -198.9286346435547, 'eval_logps/rejected': -275.6725158691406, 'eval_logits/chosen': -1.7266769409179688, 'eval_logits/rejected': -1.7001497745513916, 'epoch': 5.74}\n",
            " 80% 80/100 [22:36<03:41, 11.09s/it]\n",
            "100% 160/160 [01:08<00:00,  2.18it/s]\u001b[A\n",
            "{'loss': 0.0003, 'grad_norm': 0.010458435863256454, 'learning_rate': 0.0004, 'rewards/chosen': 0.2983384132385254, 'rewards/rejected': -9.473671913146973, 'rewards/accuracies': 1.0, 'rewards/margins': 9.772010803222656, 'logps/chosen': -135.36416625976562, 'logps/rejected': -282.6441955566406, 'logits/chosen': -1.884993314743042, 'logits/rejected': -1.5820212364196777, 'epoch': 5.81}\n",
            "{'loss': 0.0004, 'grad_norm': 0.013183824717998505, 'learning_rate': 0.00040500000000000003, 'rewards/chosen': 1.6686064004898071, 'rewards/rejected': -9.059694290161133, 'rewards/accuracies': 1.0, 'rewards/margins': 10.728300094604492, 'logps/chosen': -175.35055541992188, 'logps/rejected': -249.17404174804688, 'logits/chosen': -1.904836893081665, 'logits/rejected': -1.8481526374816895, 'epoch': 5.89}\n",
            "{'loss': 0.0003, 'grad_norm': 0.01313126552850008, 'learning_rate': 0.00041, 'rewards/chosen': 1.9974571466445923, 'rewards/rejected': -7.985390663146973, 'rewards/accuracies': 1.0, 'rewards/margins': 9.982847213745117, 'logps/chosen': -211.89132690429688, 'logps/rejected': -237.99188232421875, 'logits/chosen': -1.786277413368225, 'logits/rejected': -1.6560750007629395, 'epoch': 5.96}\n",
            "{'loss': 0.0001, 'grad_norm': 0.013801292516291142, 'learning_rate': 0.000415, 'rewards/chosen': 3.1598639488220215, 'rewards/rejected': -5.33986234664917, 'rewards/accuracies': 1.0, 'rewards/margins': 8.499726295471191, 'logps/chosen': -245.370361328125, 'logps/rejected': -136.18191528320312, 'logits/chosen': -1.3442907333374023, 'logits/rejected': -1.648911476135254, 'epoch': 6.0}\n",
            "{'loss': 0.0002, 'grad_norm': 0.008305947296321392, 'learning_rate': 0.00042, 'rewards/chosen': 2.2238609790802, 'rewards/rejected': -10.71748161315918, 'rewards/accuracies': 1.0, 'rewards/margins': 12.9413423538208, 'logps/chosen': -182.64810180664062, 'logps/rejected': -275.91571044921875, 'logits/chosen': -1.7458502054214478, 'logits/rejected': -1.548761010169983, 'epoch': 6.07}\n",
            "{'loss': 0.0002, 'grad_norm': 0.009609349071979523, 'learning_rate': 0.000425, 'rewards/chosen': 1.9137229919433594, 'rewards/rejected': -7.36113977432251, 'rewards/accuracies': 1.0, 'rewards/margins': 9.274862289428711, 'logps/chosen': -197.9857177734375, 'logps/rejected': -189.96673583984375, 'logits/chosen': -1.81317138671875, 'logits/rejected': -1.703412652015686, 'epoch': 6.15}\n",
            "{'loss': 0.0001, 'grad_norm': 0.006156467832624912, 'learning_rate': 0.00043, 'rewards/chosen': 0.7570337653160095, 'rewards/rejected': -9.366159439086914, 'rewards/accuracies': 1.0, 'rewards/margins': 10.123193740844727, 'logps/chosen': -242.09788513183594, 'logps/rejected': -271.7735900878906, 'logits/chosen': -1.7579537630081177, 'logits/rejected': -1.872354507446289, 'epoch': 6.22}\n",
            "{'loss': 0.0001, 'grad_norm': 0.003085311036556959, 'learning_rate': 0.000435, 'rewards/chosen': 0.7399450540542603, 'rewards/rejected': -9.959174156188965, 'rewards/accuracies': 1.0, 'rewards/margins': 10.699119567871094, 'logps/chosen': -143.46682739257812, 'logps/rejected': -283.5970764160156, 'logits/chosen': -1.8763405084609985, 'logits/rejected': -1.8549410104751587, 'epoch': 6.3}\n",
            "{'loss': 0.0001, 'grad_norm': 0.002976643620058894, 'learning_rate': 0.00044, 'rewards/chosen': 0.0652303695678711, 'rewards/rejected': -12.16296100616455, 'rewards/accuracies': 1.0, 'rewards/margins': 12.228190422058105, 'logps/chosen': -145.22251892089844, 'logps/rejected': -388.50634765625, 'logits/chosen': -1.9571797847747803, 'logits/rejected': -1.5020782947540283, 'epoch': 6.37}\n",
            "{'loss': 0.0, 'grad_norm': 0.0025802955497056246, 'learning_rate': 0.00044500000000000003, 'rewards/chosen': -0.35461798310279846, 'rewards/rejected': -11.805185317993164, 'rewards/accuracies': 1.0, 'rewards/margins': 11.450567245483398, 'logps/chosen': -264.88165283203125, 'logps/rejected': -356.89385986328125, 'logits/chosen': -1.8376531600952148, 'logits/rejected': -1.863642930984497, 'epoch': 6.44}\n",
            " 90% 90/100 [24:12<01:49, 10.99s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:44,  3.58it/s]\u001b[A\n",
            "  2% 3/160 [00:01<01:00,  2.62it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:56,  2.78it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:05,  2.35it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:02,  2.45it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:06,  2.31it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:08,  2.22it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:06,  2.25it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:11,  2.11it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.33it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.43it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:04,  2.29it/s]\u001b[A\n",
            "  9% 14/160 [00:06<01:08,  2.13it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:09,  2.09it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.24it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.45it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.24it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.44it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.61it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.65it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.44it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:50,  2.70it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:50,  2.72it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:49,  2.72it/s]\u001b[A\n",
            " 16% 26/160 [00:10<00:59,  2.25it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:07,  1.98it/s]\u001b[A\n",
            " 18% 28/160 [00:11<01:00,  2.18it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:53,  2.43it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:52,  2.47it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:58,  2.21it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:53,  2.41it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:48,  2.60it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:46,  2.73it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.72it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:42,  2.94it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:42,  2.87it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.78it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:42,  2.88it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:44,  2.67it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.55it/s]\u001b[A\n",
            " 26% 42/160 [00:17<00:45,  2.57it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:50,  2.31it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.13it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.27it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:54,  2.11it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.07it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.45it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.53it/s]\u001b[A\n",
            " 32% 51/160 [00:21<00:44,  2.44it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:48,  2.22it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:49,  2.16it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:44,  2.38it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:44,  2.34it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:43,  2.38it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:42,  2.42it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:37,  2.72it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:37,  2.68it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:37,  2.63it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:41,  2.41it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:43,  2.28it/s]\u001b[A\n",
            " 39% 63/160 [00:26<00:40,  2.39it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:44,  2.18it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.39it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.58it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:40,  2.32it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.18it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.71it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.36it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.19it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.31it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.68it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.45it/s]\u001b[A\n",
            " 48% 77/160 [00:32<00:37,  2.22it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.17it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.47it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:32,  2.43it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:32,  2.41it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:35,  2.21it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.33it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:32,  2.31it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:34,  2.19it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:32,  2.31it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:32,  2.27it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:29,  2.43it/s]\u001b[A\n",
            " 56% 89/160 [00:37<00:28,  2.52it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:26,  2.68it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.89it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.48it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.74it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:23,  2.84it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.65it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.43it/s]\u001b[A\n",
            " 61% 97/160 [00:40<00:24,  2.61it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.48it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.42it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.16it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.05it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.04it/s]\u001b[A\n",
            " 66% 106/160 [00:44<00:23,  2.28it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:22,  2.40it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.18it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.22it/s]\u001b[A\n",
            " 69% 110/160 [00:46<00:24,  2.00it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.94it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.83it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  1.99it/s]\u001b[A\n",
            " 71% 114/160 [00:48<00:24,  1.90it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:21,  2.12it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:19,  2.28it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.27it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:19,  2.21it/s]\u001b[A\n",
            " 74% 119/160 [00:50<00:16,  2.42it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:18,  2.20it/s]\u001b[A\n",
            " 76% 121/160 [00:51<00:17,  2.21it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.41it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.27it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:15,  2.39it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:14,  2.47it/s]\u001b[A\n",
            " 79% 126/160 [00:53<00:14,  2.32it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.42it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.51it/s]\u001b[A\n",
            " 81% 129/160 [00:54<00:11,  2.66it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.35it/s]\u001b[A\n",
            " 82% 131/160 [00:55<00:12,  2.35it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:12,  2.16it/s]\u001b[A\n",
            " 83% 133/160 [00:56<00:12,  2.20it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.49it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.66it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.34it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:09,  2.33it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.31it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:08,  2.41it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:07,  2.55it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.44it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.47it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.29it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.52it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.60it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.51it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.34it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.45it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.10it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:04,  2.25it/s]\u001b[A\n",
            " 95% 152/160 [01:04<00:03,  2.10it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.16it/s]\u001b[A\n",
            " 96% 154/160 [01:05<00:02,  2.30it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.41it/s]\u001b[A\n",
            " 98% 156/160 [01:06<00:01,  2.04it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.03it/s]\u001b[A\n",
            " 99% 158/160 [01:07<00:00,  2.03it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.09it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.00011547830945346504, 'eval_runtime': 68.2048, 'eval_samples_per_second': 2.346, 'eval_steps_per_second': 2.346, 'eval_rewards/chosen': 1.287205696105957, 'eval_rewards/rejected': -9.70435619354248, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 10.991562843322754, 'eval_logps/chosen': -200.202392578125, 'eval_logps/rejected': -282.4648742675781, 'eval_logits/chosen': -1.824323296546936, 'eval_logits/rejected': -1.7983382940292358, 'epoch': 6.44}\n",
            " 90% 90/100 [25:21<01:49, 10.99s/it]\n",
            "100% 160/160 [01:07<00:00,  2.17it/s]\u001b[A\n",
            "{'loss': 0.0001, 'grad_norm': 0.005206102970987558, 'learning_rate': 0.00045000000000000004, 'rewards/chosen': 2.3147284984588623, 'rewards/rejected': -7.195123672485352, 'rewards/accuracies': 1.0, 'rewards/margins': 9.50985336303711, 'logps/chosen': -198.5216064453125, 'logps/rejected': -214.99935913085938, 'logits/chosen': -1.9858920574188232, 'logits/rejected': -1.9858877658843994, 'epoch': 6.52}\n",
            "{'loss': 0.0004, 'grad_norm': 0.011695553548634052, 'learning_rate': 0.000455, 'rewards/chosen': 0.81244957447052, 'rewards/rejected': -8.334325790405273, 'rewards/accuracies': 1.0, 'rewards/margins': 9.14677619934082, 'logps/chosen': -175.38595581054688, 'logps/rejected': -219.81578063964844, 'logits/chosen': -1.8020203113555908, 'logits/rejected': -1.7611010074615479, 'epoch': 6.59}\n",
            "{'loss': 0.0001, 'grad_norm': 0.004282237030565739, 'learning_rate': 0.00046, 'rewards/chosen': 1.5397155284881592, 'rewards/rejected': -8.721369743347168, 'rewards/accuracies': 1.0, 'rewards/margins': 10.26108455657959, 'logps/chosen': -179.97230529785156, 'logps/rejected': -245.50875854492188, 'logits/chosen': -1.7865283489227295, 'logits/rejected': -1.8164176940917969, 'epoch': 6.67}\n",
            "{'loss': 0.0001, 'grad_norm': 0.004222742281854153, 'learning_rate': 0.000465, 'rewards/chosen': 0.2647022008895874, 'rewards/rejected': -11.88884162902832, 'rewards/accuracies': 1.0, 'rewards/margins': 12.153543472290039, 'logps/chosen': -228.0059356689453, 'logps/rejected': -404.6969299316406, 'logits/chosen': -1.5165222883224487, 'logits/rejected': -1.386325478553772, 'epoch': 6.74}\n",
            "{'loss': 0.0, 'grad_norm': 0.0018600296461954713, 'learning_rate': 0.00047, 'rewards/chosen': 1.4605754613876343, 'rewards/rejected': -10.287742614746094, 'rewards/accuracies': 1.0, 'rewards/margins': 11.748318672180176, 'logps/chosen': -210.36544799804688, 'logps/rejected': -323.32733154296875, 'logits/chosen': -1.8368409872055054, 'logits/rejected': -1.8549301624298096, 'epoch': 6.81}\n",
            "{'loss': 0.0001, 'grad_norm': 0.004111945163458586, 'learning_rate': 0.000475, 'rewards/chosen': 2.530668258666992, 'rewards/rejected': -9.513893127441406, 'rewards/accuracies': 1.0, 'rewards/margins': 12.044561386108398, 'logps/chosen': -198.07012939453125, 'logps/rejected': -279.0462646484375, 'logits/chosen': -1.8212921619415283, 'logits/rejected': -1.6953201293945312, 'epoch': 6.89}\n",
            "{'loss': 0.0001, 'grad_norm': 0.007795493584126234, 'learning_rate': 0.00048, 'rewards/chosen': 2.7621192932128906, 'rewards/rejected': -7.632566928863525, 'rewards/accuracies': 1.0, 'rewards/margins': 10.394685745239258, 'logps/chosen': -235.50442504882812, 'logps/rejected': -219.9954833984375, 'logits/chosen': -1.5959479808807373, 'logits/rejected': -1.9066890478134155, 'epoch': 6.96}\n",
            "{'loss': 0.0001, 'grad_norm': 0.008643567562103271, 'learning_rate': 0.00048499999999999997, 'rewards/chosen': -1.6249908208847046, 'rewards/rejected': -12.84260368347168, 'rewards/accuracies': 1.0, 'rewards/margins': 11.217613220214844, 'logps/chosen': -341.2861633300781, 'logps/rejected': -331.3049621582031, 'logits/chosen': -1.8196241855621338, 'logits/rejected': -2.0940284729003906, 'epoch': 7.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.002889710245653987, 'learning_rate': 0.00049, 'rewards/chosen': -0.5039210319519043, 'rewards/rejected': -12.026701927185059, 'rewards/accuracies': 1.0, 'rewards/margins': 11.52277946472168, 'logps/chosen': -202.03549194335938, 'logps/rejected': -351.91558837890625, 'logits/chosen': -1.7956609725952148, 'logits/rejected': -1.7081626653671265, 'epoch': 7.07}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00286000594496727, 'learning_rate': 0.000495, 'rewards/chosen': -0.4083903431892395, 'rewards/rejected': -10.728415489196777, 'rewards/accuracies': 1.0, 'rewards/margins': 10.320026397705078, 'logps/chosen': -222.20846557617188, 'logps/rejected': -313.17523193359375, 'logits/chosen': -1.9561173915863037, 'logits/rejected': -1.9181939363479614, 'epoch': 7.15}\n",
            "100% 100/100 [27:00<00:00, 10.79s/it]\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/160 [00:00<00:42,  3.74it/s]\u001b[A\n",
            "  2% 3/160 [00:01<00:57,  2.75it/s]\u001b[A\n",
            "  2% 4/160 [00:01<00:54,  2.86it/s]\u001b[A\n",
            "  3% 5/160 [00:01<01:05,  2.37it/s]\u001b[A\n",
            "  4% 6/160 [00:02<01:02,  2.48it/s]\u001b[A\n",
            "  4% 7/160 [00:02<01:06,  2.32it/s]\u001b[A\n",
            "  5% 8/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 9/160 [00:03<01:07,  2.24it/s]\u001b[A\n",
            "  6% 10/160 [00:04<01:11,  2.11it/s]\u001b[A\n",
            "  7% 11/160 [00:04<01:03,  2.34it/s]\u001b[A\n",
            "  8% 12/160 [00:04<01:00,  2.44it/s]\u001b[A\n",
            "  8% 13/160 [00:05<01:03,  2.30it/s]\u001b[A\n",
            "  9% 14/160 [00:05<01:08,  2.13it/s]\u001b[A\n",
            "  9% 15/160 [00:06<01:08,  2.11it/s]\u001b[A\n",
            " 10% 16/160 [00:06<01:04,  2.24it/s]\u001b[A\n",
            " 11% 17/160 [00:07<00:58,  2.46it/s]\u001b[A\n",
            " 11% 18/160 [00:07<01:03,  2.25it/s]\u001b[A\n",
            " 12% 19/160 [00:08<00:57,  2.45it/s]\u001b[A\n",
            " 12% 20/160 [00:08<00:53,  2.63it/s]\u001b[A\n",
            " 13% 21/160 [00:08<00:52,  2.67it/s]\u001b[A\n",
            " 14% 22/160 [00:09<00:56,  2.46it/s]\u001b[A\n",
            " 14% 23/160 [00:09<00:51,  2.66it/s]\u001b[A\n",
            " 15% 24/160 [00:09<00:52,  2.60it/s]\u001b[A\n",
            " 16% 25/160 [00:10<00:52,  2.60it/s]\u001b[A\n",
            " 16% 26/160 [00:10<01:01,  2.17it/s]\u001b[A\n",
            " 17% 27/160 [00:11<01:07,  1.97it/s]\u001b[A\n",
            " 18% 28/160 [00:11<00:59,  2.20it/s]\u001b[A\n",
            " 18% 29/160 [00:12<00:52,  2.48it/s]\u001b[A\n",
            " 19% 30/160 [00:12<00:50,  2.56it/s]\u001b[A\n",
            " 19% 31/160 [00:13<00:56,  2.28it/s]\u001b[A\n",
            " 20% 32/160 [00:13<00:51,  2.47it/s]\u001b[A\n",
            " 21% 33/160 [00:13<00:48,  2.64it/s]\u001b[A\n",
            " 21% 34/160 [00:14<00:45,  2.75it/s]\u001b[A\n",
            " 22% 35/160 [00:14<00:45,  2.75it/s]\u001b[A\n",
            " 22% 36/160 [00:14<00:41,  2.98it/s]\u001b[A\n",
            " 23% 37/160 [00:15<00:42,  2.90it/s]\u001b[A\n",
            " 24% 38/160 [00:15<00:43,  2.83it/s]\u001b[A\n",
            " 24% 39/160 [00:15<00:41,  2.91it/s]\u001b[A\n",
            " 25% 40/160 [00:16<00:45,  2.66it/s]\u001b[A\n",
            " 26% 41/160 [00:16<00:46,  2.53it/s]\u001b[A\n",
            " 26% 42/160 [00:16<00:46,  2.54it/s]\u001b[A\n",
            " 27% 43/160 [00:17<00:51,  2.28it/s]\u001b[A\n",
            " 28% 44/160 [00:18<00:54,  2.12it/s]\u001b[A\n",
            " 28% 45/160 [00:18<00:50,  2.27it/s]\u001b[A\n",
            " 29% 46/160 [00:19<00:54,  2.10it/s]\u001b[A\n",
            " 29% 47/160 [00:19<00:54,  2.08it/s]\u001b[A\n",
            " 30% 48/160 [00:19<00:47,  2.36it/s]\u001b[A\n",
            " 31% 49/160 [00:20<00:45,  2.45it/s]\u001b[A\n",
            " 31% 50/160 [00:20<00:43,  2.53it/s]\u001b[A\n",
            " 32% 51/160 [00:20<00:44,  2.46it/s]\u001b[A\n",
            " 32% 52/160 [00:21<00:49,  2.19it/s]\u001b[A\n",
            " 33% 53/160 [00:22<00:50,  2.13it/s]\u001b[A\n",
            " 34% 54/160 [00:22<00:45,  2.32it/s]\u001b[A\n",
            " 34% 55/160 [00:22<00:46,  2.27it/s]\u001b[A\n",
            " 35% 56/160 [00:23<00:43,  2.37it/s]\u001b[A\n",
            " 36% 57/160 [00:23<00:42,  2.45it/s]\u001b[A\n",
            " 36% 58/160 [00:23<00:36,  2.79it/s]\u001b[A\n",
            " 37% 59/160 [00:24<00:36,  2.78it/s]\u001b[A\n",
            " 38% 60/160 [00:24<00:36,  2.76it/s]\u001b[A\n",
            " 38% 61/160 [00:25<00:39,  2.49it/s]\u001b[A\n",
            " 39% 62/160 [00:25<00:42,  2.33it/s]\u001b[A\n",
            " 39% 63/160 [00:25<00:39,  2.43it/s]\u001b[A\n",
            " 40% 64/160 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 41% 65/160 [00:26<00:39,  2.41it/s]\u001b[A\n",
            " 41% 66/160 [00:27<00:36,  2.60it/s]\u001b[A\n",
            " 42% 67/160 [00:27<00:39,  2.33it/s]\u001b[A\n",
            " 42% 68/160 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 43% 69/160 [00:28<00:41,  2.18it/s]\u001b[A\n",
            " 44% 70/160 [00:28<00:35,  2.55it/s]\u001b[A\n",
            " 44% 71/160 [00:29<00:32,  2.71it/s]\u001b[A\n",
            " 45% 72/160 [00:29<00:37,  2.36it/s]\u001b[A\n",
            " 46% 73/160 [00:30<00:39,  2.19it/s]\u001b[A\n",
            " 46% 74/160 [00:30<00:37,  2.31it/s]\u001b[A\n",
            " 47% 75/160 [00:30<00:31,  2.68it/s]\u001b[A\n",
            " 48% 76/160 [00:31<00:34,  2.45it/s]\u001b[A\n",
            " 48% 77/160 [00:31<00:37,  2.22it/s]\u001b[A\n",
            " 49% 78/160 [00:32<00:37,  2.17it/s]\u001b[A\n",
            " 49% 79/160 [00:32<00:32,  2.47it/s]\u001b[A\n",
            " 50% 80/160 [00:33<00:33,  2.42it/s]\u001b[A\n",
            " 51% 81/160 [00:33<00:33,  2.36it/s]\u001b[A\n",
            " 51% 82/160 [00:34<00:36,  2.16it/s]\u001b[A\n",
            " 52% 83/160 [00:34<00:33,  2.27it/s]\u001b[A\n",
            " 52% 84/160 [00:34<00:33,  2.24it/s]\u001b[A\n",
            " 53% 85/160 [00:35<00:34,  2.18it/s]\u001b[A\n",
            " 54% 86/160 [00:35<00:31,  2.32it/s]\u001b[A\n",
            " 54% 87/160 [00:36<00:31,  2.32it/s]\u001b[A\n",
            " 55% 88/160 [00:36<00:28,  2.52it/s]\u001b[A\n",
            " 56% 89/160 [00:36<00:27,  2.59it/s]\u001b[A\n",
            " 56% 90/160 [00:37<00:25,  2.74it/s]\u001b[A\n",
            " 57% 91/160 [00:37<00:23,  2.96it/s]\u001b[A\n",
            " 57% 92/160 [00:38<00:27,  2.50it/s]\u001b[A\n",
            " 58% 93/160 [00:38<00:24,  2.76it/s]\u001b[A\n",
            " 59% 94/160 [00:38<00:22,  2.87it/s]\u001b[A\n",
            " 59% 95/160 [00:39<00:24,  2.66it/s]\u001b[A\n",
            " 60% 96/160 [00:39<00:26,  2.43it/s]\u001b[A\n",
            " 61% 97/160 [00:39<00:24,  2.61it/s]\u001b[A\n",
            " 61% 98/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 99/160 [00:40<00:22,  2.76it/s]\u001b[A\n",
            " 62% 100/160 [00:41<00:24,  2.49it/s]\u001b[A\n",
            " 63% 101/160 [00:41<00:24,  2.42it/s]\u001b[A\n",
            " 64% 102/160 [00:42<00:27,  2.11it/s]\u001b[A\n",
            " 64% 103/160 [00:42<00:26,  2.17it/s]\u001b[A\n",
            " 65% 104/160 [00:43<00:27,  2.06it/s]\u001b[A\n",
            " 66% 105/160 [00:43<00:26,  2.06it/s]\u001b[A\n",
            " 66% 106/160 [00:43<00:23,  2.29it/s]\u001b[A\n",
            " 67% 107/160 [00:44<00:21,  2.41it/s]\u001b[A\n",
            " 68% 108/160 [00:44<00:23,  2.22it/s]\u001b[A\n",
            " 68% 109/160 [00:45<00:22,  2.23it/s]\u001b[A\n",
            " 69% 110/160 [00:45<00:25,  1.99it/s]\u001b[A\n",
            " 69% 111/160 [00:46<00:25,  1.91it/s]\u001b[A\n",
            " 70% 112/160 [00:47<00:26,  1.82it/s]\u001b[A\n",
            " 71% 113/160 [00:47<00:23,  2.01it/s]\u001b[A\n",
            " 71% 114/160 [00:47<00:23,  1.95it/s]\u001b[A\n",
            " 72% 115/160 [00:48<00:20,  2.20it/s]\u001b[A\n",
            " 72% 116/160 [00:48<00:18,  2.35it/s]\u001b[A\n",
            " 73% 117/160 [00:49<00:18,  2.32it/s]\u001b[A\n",
            " 74% 118/160 [00:49<00:18,  2.24it/s]\u001b[A\n",
            " 74% 119/160 [00:49<00:16,  2.45it/s]\u001b[A\n",
            " 75% 120/160 [00:50<00:17,  2.23it/s]\u001b[A\n",
            " 76% 121/160 [00:50<00:17,  2.25it/s]\u001b[A\n",
            " 76% 122/160 [00:51<00:15,  2.45it/s]\u001b[A\n",
            " 77% 123/160 [00:51<00:16,  2.31it/s]\u001b[A\n",
            " 78% 124/160 [00:52<00:14,  2.41it/s]\u001b[A\n",
            " 78% 125/160 [00:52<00:13,  2.51it/s]\u001b[A\n",
            " 79% 126/160 [00:52<00:14,  2.35it/s]\u001b[A\n",
            " 79% 127/160 [00:53<00:13,  2.43it/s]\u001b[A\n",
            " 80% 128/160 [00:53<00:12,  2.51it/s]\u001b[A\n",
            " 81% 129/160 [00:53<00:11,  2.68it/s]\u001b[A\n",
            " 81% 130/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 131/160 [00:54<00:12,  2.36it/s]\u001b[A\n",
            " 82% 132/160 [00:55<00:13,  2.15it/s]\u001b[A\n",
            " 83% 133/160 [00:55<00:12,  2.18it/s]\u001b[A\n",
            " 84% 134/160 [00:56<00:10,  2.48it/s]\u001b[A\n",
            " 84% 135/160 [00:56<00:09,  2.65it/s]\u001b[A\n",
            " 85% 136/160 [00:57<00:10,  2.30it/s]\u001b[A\n",
            " 86% 137/160 [00:57<00:10,  2.22it/s]\u001b[A\n",
            " 86% 138/160 [00:58<00:09,  2.21it/s]\u001b[A\n",
            " 87% 139/160 [00:58<00:09,  2.29it/s]\u001b[A\n",
            " 88% 140/160 [00:58<00:08,  2.47it/s]\u001b[A\n",
            " 88% 141/160 [00:59<00:07,  2.42it/s]\u001b[A\n",
            " 89% 142/160 [00:59<00:07,  2.49it/s]\u001b[A\n",
            " 89% 143/160 [01:00<00:07,  2.33it/s]\u001b[A\n",
            " 90% 144/160 [01:00<00:06,  2.60it/s]\u001b[A\n",
            " 91% 145/160 [01:00<00:05,  2.66it/s]\u001b[A\n",
            " 91% 146/160 [01:01<00:05,  2.53it/s]\u001b[A\n",
            " 92% 147/160 [01:01<00:05,  2.36it/s]\u001b[A\n",
            " 92% 148/160 [01:02<00:04,  2.44it/s]\u001b[A\n",
            " 93% 149/160 [01:02<00:04,  2.40it/s]\u001b[A\n",
            " 94% 150/160 [01:03<00:04,  2.10it/s]\u001b[A\n",
            " 94% 151/160 [01:03<00:03,  2.25it/s]\u001b[A\n",
            " 95% 152/160 [01:03<00:03,  2.10it/s]\u001b[A\n",
            " 96% 153/160 [01:04<00:03,  2.15it/s]\u001b[A\n",
            " 96% 154/160 [01:04<00:02,  2.27it/s]\u001b[A\n",
            " 97% 155/160 [01:05<00:02,  2.39it/s]\u001b[A\n",
            " 98% 156/160 [01:05<00:01,  2.04it/s]\u001b[A\n",
            " 98% 157/160 [01:06<00:01,  2.04it/s]\u001b[A\n",
            " 99% 158/160 [01:06<00:00,  2.03it/s]\u001b[A\n",
            " 99% 159/160 [01:07<00:00,  2.10it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 7.369514787569642e-05, 'eval_runtime': 67.9688, 'eval_samples_per_second': 2.354, 'eval_steps_per_second': 2.354, 'eval_rewards/chosen': 1.160078763961792, 'eval_rewards/rejected': -10.277261734008789, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 11.437341690063477, 'eval_logps/chosen': -201.47366333007812, 'eval_logps/rejected': -288.1940002441406, 'eval_logits/chosen': -1.893439531326294, 'eval_logits/rejected': -1.8678869009017944, 'epoch': 7.15}\n",
            "100% 100/100 [28:08<00:00, 10.79s/it]\n",
            "100% 160/160 [01:07<00:00,  2.17it/s]\u001b[A\n",
            "{'train_runtime': 1688.8791, 'train_samples_per_second': 0.711, 'train_steps_per_second': 0.059, 'train_loss': 0.1734614363608489, 'epoch': 7.15}\n",
            "100% 100/100 [28:08<00:00, 16.89s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =     7.1481\n",
            "  total_flos               =        0GF\n",
            "  train_loss               =     0.1735\n",
            "  train_runtime            = 0:28:08.87\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =      0.711\n",
            "  train_steps_per_second   =      0.059\n",
            "\u001b[32m2026-01-23 06:23:02.069\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 1688.8791, 'train_samples_per_second': 0.711, 'train_steps_per_second': 0.059, 'total_flos': 0.0, 'train_loss': 0.1734614363608489, 'epoch': 7.148148148148148, 'train_samples': 500}\u001b[0m\n",
            "\u001b[32m2026-01-23 06:23:02.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m483\u001b[0m - \u001b[1mSaving model checkpoint to outputs-dpo-v1\u001b[0m\n",
            "\u001b[32m2026-01-23 06:23:03.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 160/160 [01:07<00:00,  2.35it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     7.1481\n",
            "  eval_logits/chosen      =    -1.8934\n",
            "  eval_logits/rejected    =    -1.8679\n",
            "  eval_logps/chosen       =  -201.4737\n",
            "  eval_logps/rejected     =   -288.194\n",
            "  eval_loss               =     0.0001\n",
            "  eval_rewards/accuracies =        1.0\n",
            "  eval_rewards/chosen     =     1.1601\n",
            "  eval_rewards/margins    =    11.4373\n",
            "  eval_rewards/rejected   =   -10.2773\n",
            "  eval_runtime            = 0:01:08.39\n",
            "  eval_samples            =        500\n",
            "  eval_samples_per_second =      2.339\n",
            "  eval_steps_per_second   =      2.339\n",
            "\u001b[32m2026-01-23 06:24:11.760\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m497\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 7.369514787569642e-05, 'eval_runtime': 68.3994, 'eval_samples_per_second': 2.339, 'eval_steps_per_second': 2.339, 'eval_rewards/chosen': 1.160078763961792, 'eval_rewards/rejected': -10.277261734008789, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 11.437341690063477, 'eval_logps/chosen': -201.47366333007812, 'eval_logps/rejected': -288.1940002441406, 'eval_logits/chosen': -1.893439531326294, 'eval_logits/rejected': -1.8678869009017944, 'epoch': 7.148148148148148, 'eval_samples': 500}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache  \\\n",
        "    --optim adamw_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ1G8C42i0Bd",
        "outputId": "8847964d-8498-4ff3-89ce-3dbb0a8da0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root 1.1K Jan 23 06:23 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Jan 23 06:23 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Jan 23 06:23 added_tokens.json\n",
            "-rw-r--r-- 1 root root  765 Jan 23 06:24 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 06:23 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Jan 23 06:23 \u001b[0m\u001b[01;34mcheckpoint-100\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Jan 23 06:09 \u001b[01;34mcheckpoint-50\u001b[0m/\n",
            "-rw-r--r-- 1 root root  570 Jan 23 06:24 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Jan 23 06:23 merges.txt\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 06:23 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Jan 23 05:54 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  648 Jan 23 06:23 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Jan 23 06:23 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  57K Jan 23 06:23 trainer_state.json\n",
            "-rw-r--r-- 1 root root 6.7K Jan 23 06:23 training_args.bin\n",
            "-rw-r--r-- 1 root root  229 Jan 23 06:23 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Jan 23 06:23 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u0-D1cVgi0Bd"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Hs7ZOP_ai0Bd"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjyJSzWWi0Bd",
        "outputId": "727ad68f-2dda-4ebc-dc75-29026543598b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 06:24:54.393382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769149494.412753   18345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769149494.418865   18345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769149494.433791   18345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149494.433820   18345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149494.433824   18345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149494.433827   18345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 06:24:54.438340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-sft', tokenizer_path=None, lora_model='outputs-dpo-v1', resize_emb=False, output_dir='merged-dpo/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-sft\n",
            "LoRA model: outputs-dpo-v1\n",
            "Loading LoRA for causal language model\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The tokenizer you are loading from 'merged-sft' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-dpo/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9LGUJ_mi0Bj",
        "outputId": "3897dd7d-5d53-4f6b-ba17-883b3a22ab37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Jan 23 06:25 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.4K Jan 23 06:25 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Jan 23 06:25 config.json\n",
            "-rw-r--r-- 1 root root  117 Jan 23 06:25 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Jan 23 06:25 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Jan 23 06:25 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Jan 23 06:25 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Jan 23 06:25 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Jan 23 06:25 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Jan 23 06:25 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me2dyJd8i0Bj",
        "outputId": "1d9f68df-468a-4038-ce7a-0bf95f527093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.6\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IqBl-DS7i0Bj"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Mx0KgUpvi0Bj"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7vaw9DrPi0Bj"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE8HAfSxi0Bj",
        "outputId": "4612c785-68c7-4024-f4fb-029d48454f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 06:25:38.746777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769149538.766046   18547 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769149538.772040   18547 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769149538.786482   18547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149538.786504   18547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149538.786508   18547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149538.786511   18547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 06:25:38.790826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-dpo', lora_model='', tokenizer_path=None, system_prompt='', stop_str='', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
            "The tokenizer you are loading from 'merged-dpo' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Qwen2TokenizerFast(name_or_path='merged-dpo', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n",
            "Starting inference.\n",
            "Generating outputs:   0% 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "===\n",
            "Input: 介绍下北京\n",
            "Output: 您好，很高兴能为您服务！北京是一个文化古城，拥有丰富的历史和文化。以下是一些关于北京的基本信息：\n",
            "\n",
            "1. **历史与文化**：\n",
            "   - 北京历史悠久，由多个古代城市组成，包括西安的长安、洛阳的洛阳和开封的开封。每个城市都有独特的历史遗迹、文化景点和传统节日。\n",
            "   - 皇城是指北京的旧城，它是皇室和王公的居住地。皇城内有许多博物馆、宫殿和历史建筑，展示了北京的历史和文化。\n",
            "\n",
            "2. **城市中心**：\n",
            "   - 北京市中心，也称为“大栅栏”，是一个热闹的地方，是购物、餐饮、娱乐和休闲的中心。这里有许多著名的商场、咖啡馆、餐厅和酒吧，也是品尝当地美食的好地方。\n",
            "   - 北京有许多著名的商业区，如东交民安、王府井、王府井外大街和天安门广场，每个区都有自己的特色和文化元素。\n",
            "\n",
            "3. **美食**：\n",
            "   - 中国是世界上美食丰富的国家之一，北京也不例外。北京有许多著名的餐馆，提供各种美食，包括北京烤鸭、炸酱面、北京炒肝、豆汁、烤鱼和各种甜点。您可以在烹饪学校学习烹饪，也可以尝试自己制作传统菜肴。\n",
            "\n",
            "4. **交通**：\n",
            "   - 北京有多个主要的交通方式，包括地下铁道、高速公路、高速铁路和公交系统。乘坐地铁或公交是快速和经济的交通方式，也是了解北京地方的重要途径。\n",
            "\n",
            "希望这些信息能帮助您更好地了解北京。如果您需要更多信息或有其他问题，欢迎随时提问。祝您在北京的旅行愉快！\n",
            "\n",
            "===\n",
            "Input: 乙肝和丙肝的区别？\n",
            "Output: 您好，我很乐意帮助您澄清关于乙肝和丙肝的问题。首先，需要明确的是，乙型肝炎（Hepatitis B，简称乙型肝炎）和丙型肝炎（Hepatitis C，简称丙型肝炎）并不是同一种疾病，它们是不同的病毒性肝炎，由不同的肝病毒引起。\n",
            "\n",
            "### 乙型肝炎\n",
            "\n",
            "乙型肝炎主要通过血液、性接触或母婴传播。病毒主要在肝脏中繁殖，但也可以在其他组织和身体部位（如唾液、精液、阴道分泌物、胎盘、皮肤或眼睛）中传播。感染乙型肝炎病毒后，个体可以成为病毒的携带者或传染源，这意味着他们有可能通过体液传播或不安全的性行为传播病毒。\n",
            "\n",
            "### 丙型肝炎\n",
            "\n",
            "丙型肝炎主要通过血液传播，但也可以通过性接触、母婴传播或共用注射器传播。与乙型肝炎类似，感染丙型肝炎病毒后，个体也可以成为病毒的携带者或传染源。虽然丙型肝炎通常更难治疗，但它也可能导致严重的并发症，包括肝硬化和肝衰竭，这是更严重形式的肝脏损伤。\n",
            "\n",
            "总的来说，虽然乙型肝炎和丙型肝炎都由肝病毒引起，它们有不同的传播途径和后果。了解它们的区别对于采取正确的预防措施和管理策略至关重要。如果您需要更多信息或有其他问题，欢迎提问。\n",
            "\n",
            "Generating outputs: 100% 1/1 [00:15<00:00, 15.75s/it]\n",
            "Saved to ./predictions_result.jsonl, size: 2\n"
          ]
        }
      ],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oMp1h4zNi0Bj"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --base_model Qwen/Qwen2.5-0.5B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVc43NnHiIdq",
        "outputId": "22206745-a54e-48e5-8037-bc4fcb7106e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-23 06:27:55.167146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769149675.193839   19139 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769149675.199975   19139 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769149675.215339   19139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149675.215375   19139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149675.215379   19139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769149675.215382   19139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-23 06:27:55.219968: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', lora_model='', tokenizer_path=None, system_prompt='', stop_str='', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n",
            "Starting inference.\n",
            "Generating outputs:   0% 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "===\n",
            "Input: 介绍下北京\n",
            "Output: 北京是中国首都，位于中国东南部，是中国的首都城市，是中国最大的城市之一，其首都城市是北京。北京是中国的主要经济、文化、政治中心之一，也是中国最大的城市之一。北京的GDP、人均GDP、人均收入在世界上都名列前茅。北京的交通非常发达，拥有世界上最繁忙的地铁系统之一。北京的气候也相当舒适，四季分明，夏季炎热，冬季寒冷，四季如春。北京有许多著名的景点和文化景观，例如故宫、天安门广场、颐和园等。北京是中国文化、历史、艺术和科学的重要象征之一，也是世界各国的重要旅游目的地之一。北京是中国最重要的城市之一，也是中国最重要的城市之一。京\n",
            "查看全文 >>\n",
            "免责声明：本文仅代表作者个人观点，与本站无关。其原创性以及文中陈述文字和内容未经本站证实，对本文以及其中全部或者部分内容、文字的真实性、完整性、及时性本站不作任何保证或承诺，请读者仅作参考，并请自行核实相关内容。\n",
            "\n",
            "===\n",
            "Input: 乙肝和丙肝的区别？\n",
            "Output: 乙肝和丙肝是两种不同的病毒性疾病。乙肝（Hepatitis B）是由乙型肝炎病毒（Hepatitis B virus, HBV）引起的传染病，而丙肝（Hepatitis C）则是由丙型肝炎病毒（Hepatitis C virus, HCV）引起的传染病。\n",
            "乙肝和丙肝的不同之处在于它们的传播途径和症状。乙肝主要通过血液传播，如共用注射器、输血、器官移植或性接触等。在一些情况下，乙肝也可能通过母婴传播。而丙肝主要通过血液传播，如共用注射器、输血、器官移植或性接触等。丙肝的症状可能包括疲劳、食欲不振、恶心、呕吐、黄疸、肝功能异常等症状。\n",
            "乙肝和丙肝都是传染性疾病，需要及时治疗和管理，以避免进一步的健康风险。如果您怀疑自己或他人患有乙肝或丙肝，请尽快就医并接受专业的诊断和治疗。\n",
            "\n",
            "Generating outputs: 100% 1/1 [00:09<00:00,  9.06s/it]\n",
            "Saved to ./predictions_result.jsonl, size: 2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}